% Copyright 2012 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\comment}[1]{}
\newcommand{\myspace}{\mbox{ }\ \ \ \ }
\newcommand{\myspacem}{\;\;\;\;\;}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.15em}{\tiny $\,\bullet\,$}}
\newcommand{\size}[1]{\left | {#1} \right |} 
\newcommand{\order}[1]{{\cal O}(#1)}

\newcommand{\cfg}{CFG}
\newcommand{\nonterm}{\mbox{$V_{{\rm N}}$}}
\newcommand{\myterm}{\mbox{$V_{{\rm T}}$}}

\newcommand{\de}{\rightarrow}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

\newcommand{\ep}{\varepsilon}

\mathchardef\mhyphen="2D
\newcommand{\of}{\mhyphen of}

\newcommand{\ah}[1]{#1_{AH}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\es}[1]{#1_{ES}}
\newcommand{\loc}[1]{#1_{LOC}}
\newcommand{\production}[1]{#1_{RULE}}
\newcommand{\sym}[1]{#1_{SYM}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\token}[1]{#1_{TOK}}
\newcommand{\alg}[1]{\textsc{#1}}
\newcommand{\AH}{\alg{AH}}
\newcommand{\Earley}{\alg{Earley}}
\newcommand{\Marpa}{\alg{Marpa}}
\newcommand{\Rubyslippers}{\alg{Ruby$\mhyphen$Slippers}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\DeclareMathOperator{\scanned}{scanned}
\DeclareMathOperator{\Hyp}{Hyp}
\DeclareMathOperator{\opL}{L}
\DeclareMathOperator{\opChart}{chart}
\newcommand\myL[1]{\opL(#1)}
\newcommand\chart[1]{\es{\opChart(#1)}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\hyphenation{ALGOL}

\begin{document}

\title{The Marpa Recognizer}

\author{Jeffrey Kegler}
\thanks{
Copyright \copyright\ 2012 Jeffrey Kegler.
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
This paper reports describes the recognizer portion
of the Marpa algorithm.
The Marpa algorithm is a practical, and fully implemented,
algorithm for the recognition,
parsing and evaluation of context-free grammars.
Marpa's recognizer is based
an Earley's algorithm,
and merges the improvements to Earley's
from Leo~\cite{Leo1991}
and Aycock and Horspool~\cite{AH2002}.
Their combination in the marpa parse engine
has an added feature:
It makes available,
before each token is scanned,
full knowledge of the state of the parse so far.
This allows input to be altered as the parse progresses,
which can be an extremely powerful technique.
\end{abstract}

\maketitle

\section{THIS IS AN EARLY DRAFT}

This a very early draft of this paper,
and is largely unchecked.
The spirit and culture of the open source community
currently dictates that works of this kind and at
this very early stage be made
available on-line in public repositories.
The reader should see the availability of this document
as compliance with that spirit and culture,
and not as in any way suggesting or
encouraging reliance on its contents.

\section{Introduction}

Despite the promise of general context-free parsing,
and the strong academic literature behind it,
it has never been incorporated into a tool
as widely available as yacc or
regular expressions.
The Marpa project was begun to end this neglect by
pulling the best results from the literature,
and to turn them into a widely-available tool.
A stable version of this tool, Marpa::XS, was
delivered to the CPAN Perl archive on Solstice Day
in 2011.

Marpa::XS is a complete implementation of a parser
generator.
Its recognizer is built around a parse engine,
which is new, but which also owes a great debt
to previous work.

\section{Preliminaries}
\label{s:prel}

I assume familiarity with standard grammar notation
(pages 14-15 in Aho and Ullman~\cite{AU1972}).
In the past,
The type system required to support
a theory of parsing has been taken
as a challenge to the typographic
imagination,
and often resulted in one to the eyesight.
This paper will often
supplement the standard notation,
using subscripts to indicate the commonly occuring types.
In general, variable will contain a capital
letter, constants will be all lower case.
For example, $\sym{a}$ and $\sym{b}$ would be symbol constants,
while $\sym{X}$ will be a variable whose value is a symbol.

Where $ABC$ is a set of symbols,
let $ABC^\ast$ be the set of all strings formed
from those symbols.
Let $ABC^+$ be the subset of $ABC^\ast$ that
contains all of its elements that are not of zero length.

For the purposes of this paper consider,
without loss of generality,
a grammar $g$,
and a set of symbols, $alphabet$.
Call the language of $g$, $\myL{g}$,
where $\myL{g} \in alphabet^\ast$
Let its input be $w$, $w \in alphabet$.
Divide $alphabet$ into two disjoint sets,
$term$ and $nonterm$.

For the rewriting, designate a set of duples, $rules$,
where $\forall Rule \sep Rule \in rules$,
$Rule$ takes the form $\sym{l} \de r$,
where $\sym{l} \in nonterm$ and 
$r \in (nonterm \cup term)^+$. 
$sym{l}$ is referred to as the left hand side (LHS)
of $Rule$.
$r$ is referred to as the right hand side (RHS)
of $Rule$.
This definition differs from the traditional one in
that the empty RHS is not allowed.
Let one symbol $\sym{start}$, $\sym{start} \in nonterm$,
be a dedicated start symbol.

The grammar $g$ can be defined as the 4-tuple
$(term, nonterm, rules, \sym{start})$.
Without loss of generality,
it is assumed that $g$ is "augmented",
so that there is a rule $\sym{start} \de \sym{old\mhyphen start}$,
and that $\sym{start}$ is not the LHS of any other rule,
or in the RHS of any rule.

I was already noted
that no rules of $g$ is allowed to be empty -- to
have a zero-length RHS.
Further, no symbol may be a proper nullable --
all symbol must be either nulling or non-nullable.
These restrictions follow Aycock and Horspool~\cite{AH2002}.

Aycock and Horspool did allow a single empty start rule
to deal with null parses.
Marpa eliminates all need for empty rules in its grammar
by dealing with null parses as a special case.
Marpa also deals with
trivial grammars (those which recognize only the null string)
as a special case.

The elimination of empty rules and proper nullables
is done by rewriting the grammar.
This can be done without loss of generality
and, In their paper~\cite{AH2002},
Aycock and Horspool
show how to do this
without effect on the time complexity
as a function of the input size.
Very importantly,
this rewrite is done in such a way that the semantics
of the original grammar can be efficiently reconstructed
at evaluation time.

In this paper, $Earley$ will refer to the Earley's original
recognizer~\cite{Earley1970}.
$Leo$ will refer to Leo's revision of $Earley$
as described in his 1991 paper~\cite{Leo1991}.
$AH$ will refer to the Aycock and Horsool's revision
of $Earley$
as described in their 2002 paper~\cite{AH2002}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},g}$ will be the language accepted by $\alg{Recce}$
when parsing grammar $g$.

\section{AHFA States}
\label{s:AHFA}

In this paper a
"split LR(0) $\epsilon$-DFA"
a described by Aycock and Horspool~\cite{AH2002},
will be called an Aycock-Horspool Finite Automaton,
or AHFA.
Let FA be the AHFA derived from $G$.
as described in~\cite{AH2002}.

A full description of how to derive an AHFA in theory
can be found in \cite{AH2002},
and examples of how to derive it in practice
can be found in the code for various release of Marpa.
Here I will just summarize the central ideas behind AHFA's.

Aycock and Horspool based their AHFA's
on a few observations.
First, in practice, Earley items with the same dotted rule
often appear in groups in the Earley sets,
where the entire shares the same origin.
Second, there was already in the literature a method
for associating groups of dotted rules that often appear together
when parsing.
This method was the LR(0) DFA used in the much-studied
LALR and LR parsers.
Third, the LR(0) items that are the components of LR(0)
states are, exactly, dotted rules.
Fourth, by taking into account symbols which derive the
null string, the LR(0) DFA could be turned into an
LR(0) $\epsilon$-DFA, which would be even more effective
at grouping dotted rules which occur together.

AHFA states are sets of dotted rules.
Aycock and Horspool realized that by changing Earley items
to track AHFA states, instead of individual dotted rules,
the size of Earley sets could be reduced,
and Earley's algorithm made faster in practice.
In short, then, AHFA states are a shorthand that Earley items
can use for groups of dotted rules that occur together frequently.
The original Earley items could be represented as $(r_{DOTTED}, origin)$
duples, where $r_{DOTTED}$ is a dotted rule.
Aycock and Horspool modified their Earley items to be $(\ah{L}, origin)$
duples, where $\ah{L}$ is an AHFA state.

\section{The Ruby Slippers Recognizer}
\label{s:recce}

\begin{algorithm}[H]
\caption{Ruby Slippers Initialization}\label{a:rs:initial}
\begin{algorithmic}[1] 
\Procedure{Initial}{$i,a$}
\State \Call{AddItems}{$0, \ah{start}, 0$}
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Ruby Slippers Scanning}\label{a:rs:scan}
\begin{algorithmic}[1] 
\Procedure{Scan}{$i,a$}
\Comment{Scan a token $a$, which starts at Earley set $i$}
\For{each Earley item $\eim{x}$ in $ES[i]$}
\State $\ah{to} \gets GOTO(AHFA\of(\eim{x}), a)$
\State \Call{Add EIM Pair}{$i+1, \ah{to}, Origin\of(\eim{x}$}
\EndFor
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Ruby Slippers Completion}\label{a:rs:complete}
\begin{algorithmic}[1] 
\Procedure{Complete}{$i_{LOC}$}
\For{each Earley item $\eim{work} \in ES[i]$}
\For{each completed rule, $\production{C}$}
\For{each $\eim{predecessor} \in ES[Origin\of \eim{work}]$}
\State $\ah{to} \gets GOTO(AHFA\of \eim{predecessor}, LHS_{SYM})$
\State \Call{Add EIM Pair}{$to_{AHFA}, Origin\of \eim{predecessor}$}
\EndFor
\EndFor
\EndFor
\State Construct $transitions[i]$
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\begin{algorithm}[H]
\caption{Ruby Slippers, Adding EIM Pairs}\label{a:rs:pair}
\begin{algorithmic}[1] 
\Function{Add EIM Pair}{$i_{LOC},\ah{discovered},origin_{LOC}$}
\State $\ah{predicted} \gets \Lambda$
\State $\eim{discovered} \gets \Lambda$
\If{$\ah{to} \neq \Lambda$}
\State $\eim{discovered} \gets [\ah{to}, origin_{LOC}]$
\If{$\eim{discovered}$ is new}
\State Add $\eim{discovered}$ to $ES[i]$
\State $\ah{predicted} \gets GOTO(\ah{to}, \epsilon)$
\EndIf
\EndIf
\If{$predicted_{AHFA} \neq \Lambda$}
\State $\eim{predicted} \gets [\ah{predicted}, i]$
\If{$eim{predicted}$ is new}
\State Add $\eim{predicted}$ to $ES[i]$
\EndIf
\EndIf
\State \textbf{Return} $\eim{discovered}$
\EndFunction
\end{algorithmic} 
\end{algorithm} 

\section{Ruby Slippers Correctness}
\label{s:leo:proof}

In this section I will show that a
Ruby Slippers
parse engine without Leo items
produces the same Earley items
as AH2002,
so that
correctness follows
from the correctness proofs
in AH2002\cite{AH2002}.
In the following,
$\es{AH[i]}$ is Earley set $i$ according
to AH2002;
$\es{RS[i]}$ is Earley set $i$ according
to the Ruby Slippers algorithm;
and $\scanned(\es{x})$ is the subset
of $\es{x}$,
that consists of all, and of only,
the its scanned Earley items.
Let the induction hypothesis, $Hyp(n)$ be
$\forall i, 0<i<n, \es{AH[i]} = \es{RS[i]}$.

\begin{lemma}\label{t:rs:correctness:basis}
$Hyp(0)$
\end{lemma}
\begin{proof}
Note that Earley set 0 never contains scanned items.
By inspection of \ref{a:rs:initial},
and comparision with AH2002, we have
$\es{AH[0]} = \es{RS[0]}$.
\end{proof}

\begin{lemma}\label{t:rs:correctness:step1}
In $\Rubyslippers$,
Let $t\mhyphen before$ be the point in time just before a call 
to
$\call{Scan}{i,\term{input[i]}}$,
and $t\mhyphen after$ the point just after.
If $\Hyp(i)$ at $t\mhyphen before$,
then at $t\mhyphen after$
$$ \Hyp(i) \wedge \chart{\AH, i+1} = \chart{\Rubyslippers, i+1} $$
\end{lemma}
\begin{proof}
From inspection of \ref{a:rs:scan}
and comparision with AH2002.
\end{proof}

\begin{lemma}\label{t:rs:correctness:step2}
\begin{multline*}
\Hyp(i) \wedge \scanned(\es{AH[i+1]}) = \scanned(\es{RS[i+1]})
\wedge \call{Complete}{i+1} \Longrightarrow \\
\Hyp(i+1)
\end{multline*}
\end{lemma}
\begin{proof}
From inspection of \ref{a:rs:complete}
and comparision with AH2002.
\end{proof}

\begin{theorem}\label{t:rs:ah-reduction}
$\forall n( 0<n<\size{input} \implies \Hyp(n))$
\end{theorem}

\begin{proof}
By induction, with \ref{t:rs:correctness:basis}
as the basis and 
\ref{t:rs:correctness:step1}
and 
\ref{t:rs:correctness:step2}
as the steps.
\end{proof}

\begin{theorem}\label{t:ah:correct}
$\myL{\AH ,g} = \myL{g}$
\end{theorem}

\begin{proof}
This is asserted in~\cite{AH2002},
and follows
from the formally stated theorems
and the detailed construction of the
AHFA presented in that paper.
\end{proof}

\begin{theorem}
$\myL{\Rubyslippers ,g} = \myL{g}$
\end{theorem}

\begin{proof}
From Theorem
\ref{t:ah:correct}
and Theorem
\ref{t:rs:ah-reduction}.
\end{proof}

\section{Ruby Slippers time complexity}
\label{s:rs:complexity}

\begin{theorem}
For all context-free grammars,
the time and space complexity
of $\Rubyslippers$ is as good or better
than that of Earley's algorithm.
\end{theorem}

\section{The Marpa recognizer}
\label{s:rs}

\section{Marpa recognizer correctness}
\label{s:rs:correctness}

\begin{theorem}
$\myL{\alg{Marpa},g} = \myL{g}$
\end{theorem}

\section{Marpa recognizer time complexity}
\label{s:marpa:complexity}

\begin{theorem}
For all context-free grammars,
the time and space complexity
of $\Marpa$ is as good or better
than that of Earley's algorithm.
\end{theorem}

\begin{theorem}
For every LR-regular grammar,
$\Marpa$ runs in linear time and linear space.
\end{theorem}

\section{Generalizing the grammar}
\label{s:generalizing}

This section deals with certain modifications
to the
definition of a grammar made by Marpa.
They
are minor from a theoretical point of view,
and their discussion has been deferred so that the
proofs up to this point might more closely follow
tradition.

First, Marpa does not require that $\symset{lh}$
and $\symset{term}$ be disjoint.
In other words a Marpa symbol may be both a terminal
and a LHS.
This expansion of the grammar definition 
is made without loss of generalization,
or effect on the results so far.

Second, Marpa's input model is a generalization of
the traditional input stream model
used so far.
Marpa's input is a set of tokens,
$toks$,
whose elements are triples of symbol,
start location and end location:
$(\sym{t}, \loc{start}, length)$
such that $\sym{t} \in \symset{term} \wedge length \ge 1 \wedge \loc{start} \ge 0$.
The size of the input, $\size{toks}$ is the maximum over
$toks$ of $\loc{start}+length$.
Tokens may overlap,
but gaps are not allowed:
$\forall \loc{i} (\exists \token{t} \sep \token{t} \in toks
\wedge t = (\sym{t}, \loc{start}, length)
\wedge \loc{start} \le \loc{i} \le \loc{start}+length$.

The traditional input stream is the special case of
a Marpa input stream where 
$\forall \token{tok} \sep \token{tok} \in toks \implies 
\token{tok} = (\sym{s}, \loc{start}, 1)
$
and
$\forall \token{tok1}, \token{tok2} \in toks \sep
\token{tok1} = (\sym{s1}, \loc{start1}, 1)
\wedge
\token{tok2} = (\sym{s2}, \loc{start2}, 1)
\wedge (\loc{start1} = \loc{start2} \implies \token{tok1} = \token{tok2})
$.

The correctness results hold for Marpa input streams,
but to preserve the time complexity bounds,
certain restriction must be imposed on them.
Call an Earley set, $\es{chart[j]}$
a "lookahead set" at $\loc{i}$
if $j>i \wedge$
and there exists some token $\token{tok} = (\sym{s}, \loc{start}, length)$
such that $\loc{start} \le i \wedge \loc{start}+length = \loc{j}$.
If, at every location,
the number of tokens which start there,
and the number of lookahead sets in play at that location
is less than a finite constant,
then Earley items can be added in amortized constant time
and the complexity results for $Ruby Slippers$ and $Marpa$
stand.

\section{Conclusion}
\label{s:conclusion}

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{JK2011}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock [Available online at http://search.cpan.org/dist/Marpa-XS/.]

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}
 
\end{document}
