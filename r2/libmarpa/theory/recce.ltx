% Copyright 2012 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt,draft]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{url}

% This is now a "paper", but may be a chapter
% or something else someday
% This command will make any such change easier.
\newcommand{\doc}{paper}

\newcommand{\todo}[1]{\par{\large\bf Todo: #1}\par}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}

\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}
\newcommand{\cat}{\,.\,}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\order}[1]{\ensuremath{{\mathcal O}(#1)}}
\newcommand{\Oc}{\order{1}}

% I use hyphens in variable names,
% so I need to ensure that subtraction is
% clearly distinguished by the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\rightarrow}
\newcommand{\destar}
    {\mathrel{\mbox{$\:\stackrel{\!{\ast}}{\Rightarrow\!}\:$}}}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

\newcommand{\set}[1]{{\left\lbrace #1 \right\rbrace} }
\newcommand{\ah}[1]{#1_{AH}}
\newcommand{\Vah}[1]{\ensuremath{\var{#1}_{AH}}}
\newcommand{\bool}[1]{\var{#1}_{BOOL}}
\newcommand{\Vbool}[1]{\ensuremath{\bool{#1}}}
\newcommand{\dr}[1]{#1_{DR}}
\newcommand{\Vdr}[1]{\ensuremath{\var{#1}_{DR}}}
\newcommand{\Vdrset}[1]{\ensuremath{\var{#1}_{\set{DR}}}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\Veim}[1]{\ensuremath{\var{#1}_{EIM}}}
\newcommand{\Veimt}[1]{\ensuremath{\var{#1}_{EIMT}}}
\newcommand{\Veimset}[1]{\ensuremath{\var{#1}_{\set{EIM}}}}
\newcommand{\Veimtset}[1]{\ensuremath{\var{#1}_{\set{EIMT}}}}
\newcommand{\es}[1]{#1_{ES}}
\newcommand{\Ves}[1]{\ensuremath{\var{#1}_{ES}}}
\newcommand{\Vest}[1]{\ensuremath{\var{#1}_{EST}}}
\newcommand{\Vesi}[2]{\ensuremath{\var{#1}[#2]_{ES}}}
\newcommand{\VVes}[2]{\ensuremath{\var{#1}[\var{#2}]_{ES}}}
\newcommand{\Vlim}[1]{\ensuremath{\var{#1}_{LIM}}}
\newcommand{\Vlimt}[1]{\ensuremath{\var{#1}_{LIMT}}}
\newcommand{\Eloc}[1]{\ensuremath{{#1}_{LOC}}}
\newcommand{\Vloc}[1]{\Eloc{\var{#1}}}
\newcommand{\Vrule}[1]{\ensuremath{\var{#1}_{RULE}}}
\newcommand{\Vruleset}[1]{\ensuremath{\var{#1}_{\set{RULE}}}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\Vstr}[1]{\ensuremath{\var{#1}_{STR}}}
\newcommand{\sym}[1]{#1_{SYM}}
\newcommand{\Vsym}[1]{\ensuremath{\var{#1}_{SYM}}}
\newcommand{\Vorig}[1]{\ensuremath{\var{#1}_{ORIG}}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\Vsymset}[1]{\ensuremath{\var{#1}_{\set{SYM}}}}
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\token}[1]{#1_{TOK}}

\newcommand{\alg}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\AH}{\ensuremath{\alg{AH}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}
\newcommand{\Emulator}{\ensuremath{\alg{Emulator}}}

\newcommand{\Vfa}{\var{fa}}
\newcommand{\Vg}{\var{g}}
\newcommand{\Vw}{\var{w}}
\newcommand{\VVw}[1]{\ensuremath{sym{\Vw[\var{#1}]}}}
\newcommand{\Vwi}[1]{\ensuremath{sym{\Vw[#1]}}}
\newcommand{\Vrules}{\var{rules}}
\newcommand{\GOTO}{\mymathop{GOTO}}
\newcommand{\Next}[1]{\mymathop{Next}(#1)}
\newcommand{\Predict}[1]{\mymathop{Predict}(#1)}
\newcommand{\Postdot}[1]{\mymathop{Postdot}(#1)}
\newcommand{\Penult}[1]{\mymathop{Penult}(#1)}
\newcommand{\myL}[1]{\mymathop{L}(#1)}
\newcommand\Etable[1]{\ensuremath{\es{\mymathop{table}[#1]}}}
\newcommand\Rtable[1]{\ensuremath{\mymathop{table}(#1)}}
\newcommand\Rtablesize[1]{\ensuremath{\big| \mymathop{table}(#1) \big|}}
\newcommand\Vtable[1]{\Etable{\var{#1}}}
\newcommand\EEtable[2]{\ensuremath{\mymathop{table}(#1,#2)}}
\newcommand\EVtable[2]{\EEtable{#1}{\var{#2}}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem{observation}[theorem]{Observation}

\hyphenation{oper-and oper-ands}

\begin{document}

\title{Marpa, a practical general parser: the recognizer}

\author{Jeffrey Kegler}
\thanks{
Copyright \copyright\ 2012 Jeffrey Kegler.
}
\thanks{
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
This \doc{} reports describes the recognizer portion
of the Marpa algorithm.
The Marpa algorithm is a practical, and fully implemented,
algorithm for the recognition,
parsing and evaluation of context-free grammars.
Based
on Earley's algorithm,
Marpa merges into it the improvements
of Leo's 1991
and Aycock and Horspool's 2002 papers.
A major feature new with Marpa is that
full knowledge of the state of the parse is available
as tokens are being scanned.
Advantageous for error detection,
this knowledge also
allows
``Ruby Slippers'' parsing --
alteration of the input in reaction
to the parser's expectations.
\end{abstract}

\maketitle

\section*{This is a draft}

This a draft.  Date: \today.

\section{Introduction}

Despite the promise of general context-free parsing,
and the strong academic literature behind it,
it has never been incorporated into a tool
as highly available as yacc\cite{Johnson} or
regular expressions.
The Marpa project was begun to end this neglect.
Its intention was to
take the best results from the literature
on Earley parsing off the pages
of the journals and bring them
to a wider audience.
A stable version of this tool, Marpa::XS\cite{Marpa-XS},
was uploaded to the CPAN Perl archive
on Solstice Day in 2011.

As implemented
Marpa parses all context-free grammars,
without exception.
Time bounds are the best of Leo\cite{Leo1991}
and Earley\cite{Earley1970}.
The Leo bound
(\Oc for LR-regular)
is especially noteworthy with respect to
Marpa's goal of being a practical parser:
If a grammar is in a class of grammar currently in practical use,
Marpa parses it in linear time.

Extremely important from the error-handling point of
view, but overlooked in the past,
are error-detection properties.
Marpa breaks new ground in this respect.
Marpa has the immediate error detection property,
but goes well beyond that:
it is fully aware of the state of the parse,
and can report this to the user while tokens are
being scanned.

Rejection of tokens in Marpa is easily and 
efficiently recoverable,
so that with Marpa error detection is no longer just
an, often desperate, last resort.
It can used as a parsing technique in itself.
Since it can be known immediately if a token
will be rejected,
the the input stream can be changed in
the light of the parser's expectations.
This means that error detection,
in previous practice,
often a desperate last resort,
becomes useful as a parsing technique.
Since approach can be described
as making the parser's
``wishes'' come true,
we have has called this
``Ruby Sippers Parsing''.

One use of the Ruby Slippers technique is to
parse with a very clean,
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
The author has implemented an HTML parser\cite{Marpa-HTML},
based on an grammar that assumes that all start
and end tags are present.
Such an HTML grammar is too simple even to describe perfectly
standard-conformant HTML,
but the the lexical analyzer is
programmed to supply start and end tags as requested by the parser.
The result is a very simply and cleanly designed parser
that parses very liberal HTML
and accepts all input files,
treating them as highly defective HTML.

Sections
\ref{s:start-prelim} through \ref{s:end-prelim}
will outline notation and some of the
underlying concepts.
Section \ref{s:pseudocode} presents the pseudocode
for the algorithm Marpa.
Section
\ref{s:proof-preliminaries}
contains the preliminaries
to the theoretical results
of this \doc{}.
Section
\ref{s:correct}
contain a proof of correctness.
Section \ref{s:complexity} contains
the complexity results.
Finally,
section \ref{s:generalization} generalizes
the definitions of grammar and input.

\section{Preliminaries}
\label{s:prel}
\label{s:start-prelim}

We assume familiarity with the basic theory of parsing,
as well as Earley's algorithm.
This \doc{} will
use subscripts to indicate its commonly occurring types.
$\var{X}_T$ will be the variable \var{X} of type $T$.
$\var{set-one}_\set{T}$ will be the variable \var{X} of type set of $T$.
and \Vsym{a} will be the variable \var{a} of type $SYM$,
where $SYM$ indicates a symbol.
\Vsymset{set-two} will be the set of symbols \var{set-two}.
Subscripts may be omitted when the type
is obvious from the context.
The notation for
constants will not differ from that for variables.

Multi-character variable names will be common,
and operations will never be implicit.
Multiplication will be explicitly indicated:
$\var{a} \times \var{b}$.
Concatenation will be shown as $\var{a} \cat \var{b}$.
Where used,
subtraction will be clearly indicated
by context and typography:
\begin{equation*}
\var{symbol-count} \subtract \var{terminal-count}
\end{equation*}

Where \Vsymset{abced} is a set of symbols,
let $\var{abced}^\ast$ be the set of all strings
(type STR) formed
from those symbols.
Let $\var{abced}^+$ be the subset of $\var{abced}^\ast$ that
contains all of its elements that are not of zero length.

For the purposes of this \doc{} consider,
without loss of generality,
a grammar \Vg,
and a set of symbols, \Vsymset{alphabet}.
Call the language of \var{g}, $\myL{\Vg}$,
where $\myL{\Vg} \in \var{alphabet}^\ast$
Divide \Vsymset{alphabet} into two disjoint sets,
\Vsymset{lh} and \Vsymset{term}.

For the rewriting, designate a set of duples, \Vruleset{rules},
where for all \Vrule{r},
$\Vrule{r} \in \Vruleset{rules}$,
\Vrule{r} takes the form $[\Vsym{lhs} \de \Vsymset{rhs}]$,
where $\Vsym{lhs} \in \var{term}$ and
$\Vstr{rhs} \in \var{alphabet}^+$.
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vsymset{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
This definition follows \cite{AH2002},
which departs from tradition by disallowing an empty RHS.

The grammar \Vg{} can be defined as the 4-tuple
\begin{equation*}
    (\Vsymset{alphabet}, \Vsymset{term}, \var{rules}, \Vsym{start})
\end{equation*}
where \Vsymset{term} is the
set of terminal symbols,
\begin{equation*}
    \Vsymset{term} \subset \Vsymset{alphabet}.
\end{equation*}
The set of potential LHS symbols, \Vsymset{lh} is
$\var{alphabet} \setminus \var{term}$.
Without loss of generality,
it is assumed that \Vg{}
has a dedicated acceptance symbol
\begin{equation*}
\Vsym{accept}, \Vsym{accept} \in \Vsymset{lh}
\end{equation*}
and a dedicated rule,
\begin{equation*}
\Vrule{accept} = [ \Vsym{accept} \de \Vsym{start} ]
\end{equation*}
such that for every rule,
$\Vrule{x} = [ \Vsym{lhs} \de \Vsymset{rhs} ]$
\begin{center}
\begin{tabular}{ll}
1.\hspace{.5em} &
$  \Vsym{accept} = \Vsym{lhs} \implies \Vrule{x} = \Vrule{accept} $ \\
2. &
$  \Vsym{accept} \notin \Vsymset{rhs} $ \\
\end{tabular}
\end{center}

Let the input to parse be \Vw, $\Vw \in \var{alphabet}^\ast$.
Locations in the input will be of type LOC.
Let \Vsize{w} be the length of the input, counted in symbols.
Let \VVw{i},
$\VVw{i} \in \Vsymset{term}$ be character \var{i}
of the input,
$0 \le \Vloc{i} < \Vsize{w}$.

In this \doc{}, \Earley{} will refer to the Earley's original
recognizer~\cite{Earley1970}.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\AH{} will refer to the Aycock and Horspool's revision
of \Earley{}
as described in~\cite{AH2002}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},\Vg}$ will be the language accepted by $\alg{Recce}$
when parsing \Vg.

\section{Rewriting the grammar}

We have already noted
that no rules of \Vg{}
have a zero-length RHS.
Further, no rule can be nullable
and all symbols must be either nulling or non-nullable --
no symbol can be a proper nullable.
These restrictions follow Aycock and Horspool~\cite{AH2002}.
The elimination of empty rules and proper nullables
is done by rewriting the grammar and,
as shown in \cite{AH2002}
is without loss of generality.
Aycock and Horspool also discuss the evaluation issues
associated with their rewrites.

Aycock and Horspool allowed
a single empty start rule
to deal with null parses.
Marpa eliminates the need for empty rules in its grammars
by treating null parses and trivial grammars as special cases.
(Trivial grammars are those which recognize only the null string.)

Because Marpa claims to be a practical parser,
it is important to emphasize
that all grammar rewrites in this \doc{}
are done in such a way that the semantics
of the original grammar can be reconstructed
simply and efficiently at evaluation time.
As one example,
when a rewrites involves the introduction of new rule,
a semantics for the new rule can be defined to pass its operands
up to a parent rule as a list,
and the semantics of the pre-existing parent rule can
be "wrapped" so that they reassemble these lists
into operands that are well-formed
for the original semantics.

As implemented,
the Marpa parser allows users to associate
semantics with an original grammar,
which has none of the restrictions imposed
on grammars in this \doc{}.
The user of a Marpa parser 
may specify any context-free grammar,
including one with properly nullable symbols,
empty rules, etc.
The user may also specify their semantics in terms
of this original, "free-form", grammar.
Marpa implements the rewrites,
and performs its evaluation,
in such a way as to keep them invisible to
the user.
From the user's point of view,
the "free-form" of his grammar is the
one being used for the parse,
and the semantics being applied are
those he specified for it.

\section{Earley's algorithm}

Let $\Vrule{r} \in \Vrules$
be a rule,
and $\Vsize{r}$ the length of its RHS.
A dotted rule (type DR) is a duple, $[\Vrule{r}, \var{pos}]$,
where $0 < \var{pos} < \size{\Vrule{r}}$.
The position indicates the extent to which
the rule has been recognized,
and is represented with a raised dot,
so that if
$$[\Vsym{A} \de \Vsym{X} . \Vsym{Y} . \Vsym{Z}]$$
is a rule,
$$[\Vsym{A} \de \var{X} . \var{Y} \mydot \var{Z}]$$
is the rule where the dot is between \Vsym{Y}
and \Vsym{Z} ($\var{pos} = 2$).

The postdot symbol of a dotted rule is
often important.
If
\begin{equation*}
\Vdr{x} = \Vsym{A} \de \Vstr{alpha} \mydot \Vsym{last} \Vstr{beta},
\end{equation*}
then
$\Postdot{\Vdr{x}} = \Vsym{last}$.
Otherwise,
$\Postdot{\Vdr{x}} = \Lambda$.
If in the above,
\begin{equation*}
\Postdot{\Vdr{x}} \ne \Lambda \wedge \Vstr{beta} \destar \epsilon
\end{equation*}
then
$\Penult{\Vdr{x}} = \Vsym{last}$.
Otherwise,
$\Penult{\Vdr{x}} = \Lambda$.

A traditional Earley item (EIMT) is the duple
\[
    [\Vdr{dotted-rule}, \Vloc{origin}]
\]
of dotted rule and origin.
(The origin is the location where recognition of the rule
started.
It is sometimes called the "parent".)
For convenience, the type ORIG will be a synonym
for LOC, indicating that the variable designates
the origin element of an Earley item.
The traditional Earley sets are sets in the strict
sense -- duplicate Earley items are not added.

\begin{sloppypar}
An traditional Earley parser builds a table of Earley sets,
\EVtable{\Earley}{i},
$0 \le \Vloc{i} < \size{\Vw}$.
Each set is the closure of the 
initialization, scanning, reduction and prediction
operations.
These four operations
are called in this \doc{} the {\bf basic
Earley operations}.
\end{sloppypar}

The sets are built in order from 0 to $\size{\Vw}$.
Recall that \Vrule{accept} was $[ \Vsym{accept} \de \Vsym{start} ]$
Define \Vdr{accept} to be $[ \Vsym{accept} \de \Vsym{start} \mydot ]$.
\Vw{} is accepted if and only if
$$[\Vdr{accept}, 0] \in \Etable{\Vsize{\Vw}}$$

\section{Operations of the Earley algorithm}

In the following,
\Vloc{j} is the current Earley set.

\subsection{Initialization}
\label{d:initial}
The {\bf initialization} basic Earley operation only takes
place in Earley set 0.
Earley set 0 is initialized to
\begin{equation*}
\set{ [ \Vdr{initial}, 0 ] },
\Vdr{initial} = [\Vsym{accept} \de \mydot \Vsym{start} ]
\end{equation*}

\subsection{Scanning}
\label{d:scan}
\begin{sloppypar}
A {\bf scanning} basic Earley operation takes place
in all Earley sets other than Earley set 0
for each pair $[\Veimt{pred}, \Vsym{token}]$,
\begin{itemize}
\item[(i)] $\var{token} = \Vwi{j \subtract 1}$
\item[(ii)] $\Veimt{pred} \in \Etable{(\Vloc{j} \subtract 1)}$
\item[(iii)] $\Veimt{pred} = [ \Vdr{before}, \Vorig{pred} ]$
\item[(iv)] $\Postdot{\Vdr{before}} = \Vsym{token}$
\end{itemize}
A scanning operation
adds \Veimt{result} to Earley set \Vloc{j},
where
\begin{equation*}
\Veimt{result} = [ \Next{\Vdr{before}}, \Vorig{pred} ]
\end{equation*}
\Veimt{pred} is called the predecessor of the scanning operation
and \Veimt{result} is called its result.
\end{sloppypar}

\subsection{Reduction}
\label{d:reduction}
An {\bf Earley reduction} basic operation is attempted
in every Earley set
for every pair $[\Veimt{pred}, \Vsym{lhs}]$,
where there exists a \Veimt{component} such that
\begin{itemize}
\item[(i)] $\Veimt{component} \in \Vtable{j}$
\item[(ii)] $\Veimt{component} = [ \Vdr{complete}, \Vorig{component} ]$
\item[(iii)] $\Vdr{complete} = [ \Vsym{lhs} \de \Vstr{rhs} \mydot ]$
\item[(iv)] $\Veimt{pred} \in \Etable{\Vorig{component}}$
\item[(v)] $\Veimt{pred} = [ \Vdr{before}, \Vorig{pred} ]$
\item[(vi)] $\Postdot{\Vdr{before}} = \Vsym{lhs}$
\end{itemize}
A reduction operation
adds \Veimt{result} to Earley set \Vloc{j},
where
\begin{equation*}
\Veimt{result} = [ \Next{\Vdr{before}}, \Vorig{pred} ]
\end{equation*}
\Veimt{component} is called the component of the reduction operation\footnote{
The term ``component'' comes from Irons \cite{Irons}.
}
\Veimt{pred} is called the predecessor of the reduction operation
and \Veimt{result} is called its result.

\subsection{Prediction}
\label{d:prediction}
Let $\mymathop{Predict}(\Vdr{d})$
be the set of dotted rules
\begin{equation*}
\set{ [ \Vsym{L} \de \mydot \Vstr{rh} ]}
\end{equation*}
of which all of the following are true
\begin{gather*}
[ \Vsym{L} \de \Vstr{rh} ] \in \Vrules \\
\Vsym{P} \destar \Vsym{L} \cat \Vstr{alpha} \\
\Vsym{P} = \Postdot{\Vdr{d}}
\end{gather*}
A {\bf prediction} basic Earley operation is attempted
in every Earley sets
for every \Veimt{pred}
\begin{itemize}
\item[(i)] $\Veimt{pred} \in \Vtable{j}$
\item[(ii)] $\Veimt{pred} = [ \Vdr{pred}, \Vorig{pred} ]$
\end{itemize}
The prediction operation
adds the members of \Veimtset{results}
to Earley set \Vloc{j},
where
\begin{multline*}
\Veimtset{results} = \{ [ \Vdr{prediction}, \Vloc{j} ] : \\
	\Vdr{prediction} \in \Predict{\Vdr{pred}} \}
\end{multline*}
\Veimt{pred} is called the predecessor of the prediction operation.
and the members of \Veimtset{results} are its results.

\subsection{Causation}
The \dfn{causation} of an operation
is the set of its operands.
Operands may be tokens (as in the case of scans and reductions)
or EIMT's (as in every case except initialization).
An operand of an operation are also called a \dfn{cause}
of that operation.
Specifically,
the \dfn{cause} of an Earley operation is
\begin{itemize}
\item the predecessor EIMT of a scanning, reduction
or prediction operation; or
\item the component EIMT of a reduction operation; or
\item the transition symbol of a scanning or
reduction operation.
\end{itemize}

The cause of an operation is also called
the \dfn{cause} of all its result EIMT's.
An EIMT which is result of an scanning or reduction operation with a 
is called the \dfn{successor} of that operation's predecessor.

\section{The Leo algorithm}

In \cite{Leo1991}, Joop Leo presented a method for
dealing with right recursion in \order{n} time.
Leo shows that,
With his modification, Earley's algorithm
is \order{n} for all LR-regular grammars.
(LR-regular is LR where lookahead
is infinite length, but restricted to
distinguishing between regular expressions.)

Summarizing Leo's method,
it consists of spotting potential right recursions
and memoizing them.
Leo restricts the memoization to situations where
the right recursion is unambiguous.
Potential right recursions are memoized by
Earley set, using what Leo called
``transitive items''.
In this \doc{} Leo's ``transitive items'' will be called Leo items.

In each Earley set, there is at most one Leo item per symbol.
The form of Leo items
from in Leo's original paper, is the triplet
$$[ \Vdr{top}, \Vsym{transition}, \Vorig{top} ]$$
where \Vsym{transition} is the transition symbol,
and
\begin{equation*}
\Veimt{top} = [\Vdr{top}, \Vorig{top}]
\end{equation*}
is the Earley to be added on reductions over
the \Vsym{transition}.
Leo items of this form will be called
``traditional'' Leo items (type LIMT).

Leo items memoize what would otherwise be sequences
of Earley items.
Leo items only memoize unambiguous (or
deterministic) sequences,
and so that top of the sequence can stand
in for the entire sequence --
the only role the other EIMT's in the sequence would
play in a parse is to derive the top EIMT.
We will call these memoized sequences, Leo sequences.

Each Leo sequence, if fully expanded would contain \order{n} items,
where \var{n} is the length of the sequence.
In \Earley, each such sequence would be expanded in every
Earley set which is the origin of an EIMT included in the
sequence, and the total number of EIMT's would be
\order{n^2}.

With Leo memoization, a single EIMT stands in for the sequence.
There are \Oc Leo items per Earley set,
so the cost of the sequence is \Oc per Earley set,
or \order{n} for the entire sequence.
If, at evaluation time,
it is desirable to expand the Leo sequence,
only those items actually involved in the parse
need to be expanded.
All the EIMT's of a potential right-recursion 
will be in one Earley set and the number of EIMT's
will be \order{n},
so that even including expansion of the Leo sequence
for evaluation, the time and space complexity of
the sequence remain \order{n}.

Implementing Leo memoization requires
adding another basic operation.
In addition to the basic operations of \Earley,
\Leo{} include Leo transition.
For any symbol in a reduction \Vsym{lhs},
a Leo transition, if one exists, replaces
the reduction operation.

In every Earley set after the first,
for every symbol \Vsym{lhs}
where \Earley would look for pairs
$[\Veimt{pred}, \Vsym{lhs}]$
such that
\[
    \Veimt{pred} \in \EVtable{\Earley}{i} \\
    \text{and} \Penult{\var{pred}} = \Vsym{lhs}
\]
in order
to perform reduction operations,
Leo first attempts a Leo transition.
If there is a Leo transition for \Vsym{lhs} from
Earley set \var{i},
reduction on \Vsym{lhs} from Earley set \var{i}
is not attempted.

A Leo transition from \Vloc{i} over \Vsym{lhs} is possible
if and only if there exists
\Vlimt{pred}, \Vdr{top}, \Vorig{top}
such that
\begin{center}
\begin{tabular}{rl}
                 & $\Vlimt{pred} \in \EVtable{\Earley}{i}$ \\
\text{and} & $\Vlimt{pred} = [ \Vdr{top}, \Vsym{lhs}, \Vorig{top} ]$
\end{tabular}
\end{center}
If there is a Leo transition,
$[ \Vdr{top}, \Vorig{top} ]$ is added to
\EVtable{\Leo}{j}.
Otherwise, reduction is tried.

\section{The Aycock-Horspool finite automaton}
\label{s:AHFA}
\label{s:end-prelim}

In this \doc{} a
``split LR(0) $\epsilon$-DFA''
a described by Aycock and Horspool~\cite{AH2002},
will be called an Aycock-Horspool Finite Automaton,
or AHFA.
A full description of how to derive an AHFA in theory
can be found in \cite{AH2002},
and examples of how to derive it in practice
can be found in the code for Marpa\cite{Marpa-R2,Marpa-XS}.
Here I will summarize those ideas behind AHFA's
that are central to Marpa.

Aycock and Horspool based their AHFA's
on a few observations.
\begin{itemize}
\item
In practice, Earley items with the same dotted rule
often appear in groups in the Earley sets,
where the entire shares the same origin.
\item
There was already in the literature a method
for associating groups of dotted rules that often appear together
when parsing.
This method was the LR(0) DFA used in the much-studied
LALR and LR parsers.
\item
The LR(0) items that are the components of LR(0)
states are, exactly, dotted rules.
\item
By taking into account symbols which derive the
null string, the LR(0) DFA could be turned into an
LR(0) $\epsilon$-DFA, which would be even more effective
at grouping dotted rules which occur together.
\end{itemize}

AHFA states are sets of dotted rules.
Aycock and Horspool realized that by changing Earley items
to track AHFA states, instead of individual dotted rules,
the size of Earley sets could be reduced,
and Earley's algorithm made faster in practice.
In short, then, AHFA states are a shorthand that Earley items
can use for groups of dotted rules that occur together frequently.
The original Earley items could be represented as $(\Vdr{r}, origin)$
duples, where \Vdr{r} is a dotted rule.
Aycock and Horspool modified their Earley items to be $(\ah{L}, origin)$
duples, where $\ah{L}$ is an AHFA state.

\begin{definition}
A dotted rule is {\bf Marpa-valid} if and only if
it does not have a nulling postdot symbol.
\end{definition}

AHFA states are of two kinds:
{\bf Confirmed AHFA states}
contain the predicted start rule and
confirmed rules.
{\bf Predicted AHFA states}
contain predicted rules
other than the start rule.
(In \cite{AH2002} confirmed states are called the ``kernel states'',
and predicted states are called ``non-kernel states''.)

It is important to note that
an AHFA is not a partition of the dotted
rules --
a single dotted rule can occur
in more than one AHFA state.
This does not happen frequently,
but it does happens often enough,
even in practical grammars,
that the Marpa implementation has to provide for it.

What does not seem to happen in practical grammars
is that
the size of a \Marpa{} Earley set grows larger
than that of one of Earley's original sets.
It seems the AHFA is always a win,
at least for practical grammars.

An important feature \Marpa{} is that it combines
the improvements of Leo~\cite{Leo1991} and
Aycock and Horspool\cite{AH2002}.
In that respect,
the following theorem is crucial .

\begin{theorem}\label{t:leo-singleton}
If the AHFA state of
a Marpa Earley item (EIM) is the result of a
Leo reduction,
then its AHFA state contains only one dotted rule.
\end{theorem}

\begin{proof}
Since the Earley item is the result of a Leo reduction,
we know that its AHFA state contains a completed rule.
Call that completed rule, \Vdr{complete}.
Let \Vrule{c} be the rule of \Vdr{complete},
and \var{cp} its dot position.
$\var{cp} > 0$ because completions are never
predictions.

Suppose, for a reduction to absurdity,
that the AHFA state contains another dotted rule,
\Vdr{other},
$\Vdr{complete} \neq \Vdr{other}$.
Let \Vrule{o} be the rule of \Vdr{other},
and \var{op} its dot position.
AHFA construction never places a prediction in the same
AHFA state as a completion, so
\Vdr{other} is not a prediction.
Therefore, $\var{op} > 0$.

To create a contradiction, we first prove that
$\Vrule{c} \neq \Vrule{o}$,
then that
$\Vrule{c} = \Vrule{o}$.
By the construction of an AHFA
state, both dotted rules resulted from the same series
of transitions.
But the same series of transitions over the
same rule would result in the same dot position,
$\var{cp} = \var{op}$
so that if $\Vrule{c} = \Vrule{o}$,
$\Vdr{complete} = \Vdr{other}$,
which is contrary to the assumption for the reduction.
Therefore, under the assumption for the reduction,
$\Vrule{c} \neq \Vrule{o}$.

Next we show that, under the assumption for the reduction,
that $\Vrule{c} = \Vrule{o}$
follows from \Leo's uniqueness requirement.
Since both dotted rules are in the same EIM
and neither is a prediction,
both must result from transitions,
and their transitions must have been from the same Earley set.
Since they are in the same AHFA state,
by the AHFA construction,
that transition must have been
over the same transition symbol.
\Leo{} requires that, for each predecessor Earley set and transition symbol,
that a Leo transition be from a single rule.
The assumption for the reduction is that \Vdr{complete}
and \Vdr{other} are in the same Leo reduction state,
and in order for them to have obeyed the Leo uniqueness
requirement,
we have $\Vrule{c} = \Vrule{o}$.

We now have both
$\Vrule{c} = \Vrule{o}$
and
$\Vrule{c} \neq \Vrule{o}$,
completing the reduction to absurdity.
When \Vdr{complete} is in a Leo reduction EIM,
it must be the only dotted rule in that EIM.
\end{proof}

\section{The Marpa Recognizer}
\label{s:recce}
\label{s:pseudocode}

As mentioned, it is assumed that the reader
is familiar with \Earley{}
and with the description of \AH{}
in \cite{AH2002}.
The comments to the \Marpa{} pseudocode
will focus on its differences
from those two algorithms.

With the pseudocode are observations
about its space
and time complexity.
In what follows,
we will show that all time and space resources
can be charged to attempts to add Earley items,
in a way that each attempt to add an Earley item
takes amortized \Oc{} resource.
Below, when we present the complexity proofs,
we will characterize
the orders of magnitude of actual Earley items
and of attempts to add Earley items,
as well as their relationship to each other.

To achieve the
amortized \Oc{} time and space
per Earley item, we will charge for those
resources in
three ways.

\begin{itemize}
\item We can also charge \Oc{} time and space to the parse itself.
We handle the failure cases in this way.
\item We will charge resources to the Earley set.
As long as the time or space is \Oc,
the time for the Earley set can be
re-charged to an arbitrary member of the Earley set.
If the Earley set is empty, the parse will fail at this Earley set,
and the time can be charged to the parse itself.
\item We will charge resources to attempts to add Earley items.
\end{itemize}

When discussing each procedure, we will state whether
our analysis of time and space usage is inclusive, exclusive
or caller-included.
The exclusive time or space of a procedure is that
which it uses directly,
ignoring resource usage by called procedures.
Inclusive time or space of a procedure includes
resource usage by called procedures.
Caller-included time and space is that which was already
been allocated as an inclusive resource by the procedure's
caller.

\begin{algorithm}[h]
\caption{Marpa Top-level}
\begin{algorithmic}[1]
\Procedure{Main}{}
\State \Call{Initial}{}
\For{ $\var{i}, 0 \le \var{i} \le \Vsize{w}$ }
\State \Comment At this point, $\Etable{\var{x}}$ is complete, for $0 \le \var{x} < \var{i}$
\State \Call{Scan pass}{$\var{i}, \var{w}[\var{i} \subtract 1]$}
\State reject if $\size{\Etable{\var{i}}} = 0$
\State \Call{Reduction pass}{\var{i}}
\EndFor
\For{every $[\Vah{x}, 0] \in \Etable{\Vsize{w}}$}
\If{$\Vdr{accept} \in \Vah{x}$}
\State accept \Vw{} and return
\EndIf
\EndFor
\State reject \Vw{}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Top-level code}

Exclusive time and space for the loop over the Earley sets
is charged to the Earley sets,
and overhead is charged to the parse.
All these resource charges are obviously \Oc.

\subsection{Ruby Slippers parsing}
This top-level code represents a significant change
from \AH{}.
\call{Scan pass}{} and \call{Reduction pass}{}
are separated.
As a result,
when scanning of tokens which start at location \Vloc{i} begins,
the Earley sets for all locations prior to \Vloc{i} are complete.
This means that the scanning operation has available, in
the Earley sets,
full information about the current state of the parse,
including which tokens are acceptable during the scanning phase.


\begin{algorithm}[h]
\caption{Initialization}
\begin{algorithmic}[1]
\Procedure{Initial}{}
\State \Call{Add EIM Pair}{$0, \ah{start}, 0$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Initialization}
\label{p:initial-op}
Inclusive time and space is \Oc{} and
and can be charged to the parse.

\begin{algorithm}[h]
\caption{Marpa Scan pass}\label{p:scan-pass}
\begin{algorithmic}[1]
\Procedure{Scan pass}{$\Vloc{i},\Vsym{a}$}
\For{each $\Veim{predecessor} \in \var{transitions}((\var{i} \subtract 1),\var{a})$}
\State $[\Vah{from}, \Vloc{origin}] \gets \Veim{predecessor}$
\State $\Vah{to} \gets \GOTO(\Vah{from}, \Vsym{a})$
\State \Call{Add EIM Pair}{$\Vloc{i}, \Vah{to}, \Vloc{origin}$}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Scan pass}
\label{p:scan-op}

\begin{sloppypar}
\var{transitions} is a set of tables, one per Earley set,
which is indexed by symbol.
Symbol indexing is \Oc, since the number of symbols
is a constant, but
for the operation $\var{transitions}(\Vloc{l}, \Vsym{s})$
to be constant, there must be a link directly to the Earley
set.
In the case of scanning,
the lookup is always in the previous Earley set,
and it is safe to expect this can be found
in \Oc{} time.
Inclusive time and space can be charged to the
Earley item attempts.
\end{sloppypar}

\begin{algorithm}[h]
\caption{Reduction pass}
\begin{algorithmic}[1]
\Procedure{Reduction pass}{\Vloc{i}}
\State Note: This loop must include items added
\State \hspace{2.5em} by \Call{Reduce one LHS}{}
\For{each Earley item $\Veim{work} \in \Vtable{i}$}
\State $[\Vah{work}, \Vloc{origin}] \gets \Veim{work}$
\For{each lhs of a completed rule, $\Vsym{lhs}$}
\State \Call{Reduce one LHS}{\Vloc{i}, \Vloc{origin}, \Vsym{lhs}}
\EndFor
\State \Call{Memoize transitions}{\Vloc{i}}
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Reduction pass}

The loop over \Vtable{i} must also include
any items added by \call{Reduce one LHS}.
This can be done by implementing \Vtable{i} as an ordered
set and adding new items at the end.

Exclusive time is clearly \Oc{}
and may be charged to each \Veim{work}.
Overhead may be charged to the Earley set.

\begin{algorithm}[h]
\caption{Memoize transitions}
\begin{algorithmic}[1]
\Procedure{Memoize transitions}{\Vloc{i}}
\For{every \Vsym{postdot}, a postdot symbol of $\Vtable{i}$}
\If{postdot transition is unique by rule}
\State Set $\var{transitions}(\Ves{i},\Vsym{postdot})$ to
\State \hspace\algorithmicindent to an LIM
\Else
\State Set $\var{transitions}(\Ves{i},\Vsym{postdot})$ to
\State \hspace\algorithmicindent to the set of EIM's which have
\State \hspace\algorithmicindent with \Vsym{postdot} as their postdot symbol
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Memoize transitions}

The \var{transitions} table for Earley set \Vloc{i}
is built once all EIMs have been
added to Earley set \Vloc{i}.
This can be done in
a single pass over Earley set \Vloc{i},
in \Oc{} time per EIM.
The time and space required are charged to the
Earley items being examined.
Any loop overhead may be charged to the Earley set.

Of special note is that in
setting up the \var{transitions} table for the current Earley set,
the Leo items must be created.
For each Earley set,
the number of Leo items is at most
\Vsize{alphabet}.
Since the maximum number of Leo items is a constant
their space may be charged to the Earley set.

\begin{algorithm}[h]
\caption{Reduce one LHS symbol}
\begin{algorithmic}[1]
\Procedure{Reduce one LHS}{\Vloc{i}, \Vloc{origin}, \Vsym{lhs}}
\State \Comment \var{pim} is a ``postdot item'', either a LIM or an EIM
\For{each $\var{pim} \in \var{transitions}(\Vloc{origin},\Vsym{lhs})$}
\If{\var{pim} is a LIM, \Vlim{pim}}
\State $\Vlim{pred} \gets \var{pim}$
\State Perform a \Call{Leo reduction operation}{}
\State \hspace\algorithmicindent for operands \Vloc{i}, \Vlim{pred}
\Else
\State $\Veim{pred} \gets \var{pim}$
\State Perform a \Call{Earley reduction operation}{}
\State \hspace\algorithmicindent for operands \Vloc{i}, \Veim{pred}, \Vsym{lhs}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Reduce one LHS}

Inclusive time is charged to the EIM attempt.
We show that
$\var{transitions}(\Ves{origin},\Vsym{lhs})$
can be done \Oc{} time
by noting
that the number of symbols is a constant
and assuming that \Veim{x} has a link back
to its origin Earley set.
(As implemented, Marpa's
Earley items have such links.)

\begin{algorithm}[h]
\caption{Earley reduction operation}
\begin{algorithmic}[1]
\Procedure{Earley reduction operation}{\Vloc{i}, \Veim{from}, \Vsym{trans}}
\State $[\Vah{from}, \Vloc{origin}] \gets \Veim{from}$
\State $\ah{to} \gets \GOTO(\Vah{from}, \Vsym{trans})$
\State \Call{Add EIM Pair}{\Vloc{i}, \Vah{to}, \Vloc{origin}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Earley Reduction operation}
\label{p:reduction-op}

\begin{sloppypar}
Exclusive time and space is clearly \Oc.
Inclusive time and space is charged to the
calling procedure.
\end{sloppypar}

\begin{algorithm}[h]
\caption{Leo reduction operation}
\begin{algorithmic}[1]
\Procedure{Leo reduction operation}{\Vloc{i}, \Vlim{from}}
\State $[\Vah{from}, \Vsym{trans}, \Vloc{origin}] \gets \Vlim{from}$
\State $\ah{to} \gets \GOTO(\Vah{from}, \Vsym{trans})$
\State \Call{Add EIM Pair}{\Vloc{i}, \Vah{to}, \Vloc{origin}}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Leo reduction operation}
\label{p:leo-op}

Exclusive time and space is clearly \Oc.
Inclusive time and space is charged to the
calling procedure.

\begin{algorithm}[h]
\caption{Add EIM Pair}\label{a:pair}
\begin{algorithmic}[1]
\Procedure{Add EIM Pair}{$\Vloc{i},\Vah{confirmed}, \Vloc{origin}$}
\State $\Veim{confirmed} \gets [\Vah{confirmed}, \Vloc{origin}]$
\State $\Vah{predicted} \gets \GOTO(\Vah{confirmed}, \epsilon)$
\If{\Veim{confirmed} is new}
\State Add \Veim{confirmed} to \Vtable{i}
\EndIf
\If{$\Vah{predicted} \neq \Lambda$}
\State $\Veim{predicted} \gets [\Vah{predicted}, \Vloc{i}]$
\If{\Veim{predicted} is new}
\State Add \Veim{predicted} to \Vtable{i}
\EndIf
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsection{Adding a pair of Earley items}

This operation adds a confirmed EIM
item and, if it exists, the EIM for
its null-transition.
Inclusive time and space is charged to the
calling procedure.
Trivially, the space is \Oc per call.

We show that time is also \Oc{}
by singling out two non-trivial cases:
checking that an Earley item is new,
and adding it to the Earley set.
It becomes clear
that an Earley item can be added to the current
set in \Oc{} time
if Earley set is seen as a linked
list, to the head of which the new Earley item is added.

Earley items are only added if they are new,
and showing that this check can be performed in \Oc time
per EIM (and therefore per call)
is more complicated.
A data structure that allows this
to be checked in \Oc{} time is very
briefly outlined in both
\cite[p. 97]{Earley1970} and
\cite[Vol. 1, pages 326-327]{AU1972}.
The data structures used are not named in either source,
and in this \doc{} they will be called ``per-Earley set lists'',
or PSL's.
PSL's are the subject of the next section.

\subsection{Per-set lists}

A per-set list (PSL) is kept in each Earley set.
When building a new Earley set, \EVtable{\Marpa}{j},
the PSL for every previous Earley set, \EVtable{\Marpa}{i},
keeps a list of those Earley items in \EVtable{\Marpa}{j} which have \EVtable{\Marpa}{i}
as their origin.
Each PSL needs to only keep track of a maximum
number of possible Earley items.
For traditional Earley items that maximum is the number
of dotted rules.
For Marpa, that maximum is the number of AHFA states.
In both cases,
the maximum is a constant which depends on the grammar.

Clearing and rebuilding the PSL's every time
a new Earley set would take more than \Oc{},
but can be avoided.
The per-AHFA state entries can be ``stamped'',
with a link back to the Earley set
which was current when that PSL
entry was last added or updated.

Consider the case where Marpa is building \EVtable{\Marpa}{j}
an Earley item \Veim{x} is new,
where $\Veim{x} = [ \Vah{x}, \Vloc{origin} ]$.
To do this,
Marpa checks the PSL's for $\Vtable{origin}$.
Let the Earley set ``stamp'' for that PSL entry be \EVtable{\Marpa}{p}.
If the entry has never been used, $\EVtable{\Marpa}{p} = \Lambda$
If $\EVtable{\Marpa}{p} \ne \Lambda \land \EVtable{\Marpa}{p} = \EVtable{\Marpa}{j}$,
then \Veim{x} is not new,
and will not be added to the Earley set.

If $\EVtable{\Marpa}{p} = \Lambda \lor \EVtable{\Marpa}{p} \ne \EVtable{\Marpa}{j}$,
then \Veim{x} is new.
\Veim{x} is added to the Earley set,
and the PSL's ``stamp'' is updated to \EVtable{\Marpa}{j}.
As implemented,
Marpa uses PSL's for this purpose and several others.

\section{Preliminaries to the theoretical results}
\label{s:proof-preliminaries}

\subsection{Nulling symbols}
\label{s:nulling}

As noted, Marpa grammars,
without loss of generality,
contain neither empty rules or
properly nullable symbols.
This corresponds directly
to a grammar rewrite in the \Marpa{} implementation,
and its reversal during \Marpa's evaluation phase.
For the correctness and complexity proof in this section,
we assume an additional rewrite,
this time to eliminate nulling symbols.

Elimination of nulling symbols is also
without loss of generality, as can be seen
if we assume that a history
of the rewrite is kept,
and that the rewrite is reversed
after the parse.
Clearly, whether an input \Vw{} is acceptable
or will not depend on how often
a grammar \Vg{} interpolates nulling symbols into
its input stream.

In its implementation,
\Marpa's does not directly rewrite the grammar
to eliminate nulling symbols.
But nulling symbols are ignored in
creating the AHFA states,
and must be restored during \Marpa's evaluation phase,
so that the implementation and
the simplification for theory purposes in fact
track each other fairly closely.

A question for future research is whether directly
rewriting the grammar to eliminate nulling
would in fact be a better implementation.
Most or all potential
efficiency gains are already realized through
the use of the AHFA,
but the rewrite of the Earley items after
the parse would allow the nulling symbols
to be added based on full knowledge of the
result of the parse.

\subsection{Comparing Earley items}

\begin{definition}
A Marpa Earley item \dfn{corresponds}
to a traditional Earley item
$\Veimt{x} = [\Vdr{x}, \Vorig{x}]$
if and only if the Marpa Earley item is
if $\Veim{y} = [\Vah{y}, \Vorig{x}]$
and $\Vdr{x} \in \Vah{y}$.
The relation is reflexive:
A traditional Earley item, \Veimt{x}, corresponds to a
Marpa Earley item, \Veim{y}, if and only if
\Veim{y} corresponds to \Veimt{x}.
\end{definition}

\begin{definition}
A Marpa Earley set \EVtable{\Marpa}{i}
is \dfn{consistent} if and only if
all of its Earley items correspond to
traditional Earley items in 
\EVtable{\Leo}{i}.
\end{definition}

\begin{definition}
A Marpa Earley set \EVtable{\Marpa}{i}
is \dfn{complete} if and only if for every
traditional Earley item in \EVtable{\Leo}{i}
there is a corresponding Earley item in
\EVtable{\Marpa}{i}.
\end{definition}

\begin{definition}
We say that
a Marpa Earley set is \dfn{correct}
if and only that Marpa Earley set is complete
and consistent.
\end{definition}

\subsection{About AHFA states}

In the following observations,
if
\begin{equation*}
    \Vdr{d} = [ \Vsym{L} \de \Vstr{alpha} \mydot \Vsym{t} \cat \Vstr{beta} ]
\end{equation*}
then
\begin{equation*}
\Next{\Vdr{d}} =
    [ \Vsym{L} \de \Vstr{alpha} \cat \Vsym{t} \mydot \Vstr{beta} ]
\end{equation*}
be 

The following facts about AHFA's from \cite{AH2002}
will be heavily used in the following proofs.
For convenience, they are pulled out here and
restated using the conventions of this \doc{}.
An \dfn{AHFA confirmation}
is a transition from one
state to another over a non-null symbol.

\begin{observation}
\label{o:confirmed-AHFA-consistent}
AHFA confirmation is consistent.
That is, if
\begin{equation*}
    \GOTO(\Vah{from}, \Vsym{t}) = \Vah{to} \land \Vdr{to} \in \Vah{to}
\end{equation*}
then there exists \Vdr{from} such that all of the following are true
\begin{gather*}
    \Next{\Vdr{from}} = \Vdr{to} \\
    \Postdot{\Vdr{from}} = \Vsym{t} \\
    \Vdr{from} \in \Vah{from}
\end{gather*}
\end{observation}

\begin{observation}
\label{o:confirmed-AHFA-complete}
AHFA confirmation is complete.
That is, if
\begin{equation*}
    \GOTO(\Vah{from}, \Vsym{t}) = \Vah{to} \land \Vdr{from} \in \Vah{from}
\end{equation*}
then there exists \Vdr{to} such that all of the following are true
\begin{align*}
    \Next{\Vdr{from}} = \Vdr{to} \\
    \Postdot{\Vdr{from}} = \Vsym{t} \\
    \Vdr{to} \in \Vah{to}
\end{align*}
\end{observation}

An \dfn{AHFA prediction} is a transition from one
state to another over a non-null symbol.
\begin{observation}
\label{o:predicted-AHFA-consistent}
AHFA prediction is consistent.
That is, if
\begin{equation*}
    \GOTO(\Vah{from}, \epsilon) = \Vah{to} \land \Vdr{to} \in \Vah{to}
\end{equation*}
then there exists \Vdr{from} such that both of the following are true
\begin{gather*}
    \Vdr{to} \in \Predict{\Vdr{from}} \\
    \Vdr{from} \in \Vah{from}
\end{gather*}
\end{observation}

\begin{observation}
\label{o:predicted-AHFA-complete}
AHFA prediction is complete.
That is, if
\begin{equation*}
    \GOTO(\Vah{from}, \Vsym{t}) = \Vah{to} \land \Vdr{from} \in \Vah{from}
\end{equation*}
then
\begin{equation*}
\forall \Vdr{to} \quad
    (\Vdr{to} \in \Predict{\Vdr{from}}
    \implies \Vdr{to} \in \Vah{to}
    )
\end{equation*}
\end{observation}

\section{Marpa is correct}
\label{s:correct}

\begin{theorem}\label{t:table-correct}
Marpa's Earley sets are correct.
\end{theorem}

\subsection{Proof of Theorem \ref{t:table-correct}}

\begin{sloppypar}
The proof 
is by induction on the Earley sets.
The induction hypothesis is that all Earley sets
\EVtable{\Marpa}{i},
$0 \le \Vloc{i} \le \Vloc{n}$,
are complete and consistent,
and therefore correct.
We leave it as an exercise to show, as the
basis of the induction, that
\EEtable{\Marpa}{0} is complete and consistent.
\end{sloppypar}

To show the outer induction step, we show first
consistency, then completeness.
We show consistency by
an inner induction on the Marpa operations.
As the basis of the inner induction,
an empty Marpa Earley set is
consistent, trivially.
We show the step of the inner induction by cases:
\begin{itemize}
\item \Marpa{} scanning operations;
\item \Marpa{} reductions when there are no Leo reductions; and
\item Leo reductions
\end{itemize}

\subsubsection{Marpa scanning is consistent}
\label{s:scan-consistent}

For Marpa's scanning operation, we know
that the predecessor EIM is correct
by the outer induction hypothesis,
and that the token is correct
by the definitions in the preliminaries.
We know from the pseudocode for the scanning operation
that at most two EIM's will be added.
Where
\begin{gather*}
\Vah{confirmed} = \GOTO(\Vah{predecessor}, \Vsym{token}) \\
\text{and $\Vah{predicted} = \GOTO(\Vah{confirmed}, \epsilon)$,}
\end{gather*}
we will add
\begin{center}
\begin{tabular}{ll}
$[\Vah{confirmed}, \Vloc{i}]$ & if $\Vah{confirmed} \ne \Lambda$, and \\
$[\Vah{predicted}, 0]$ & if
$\Vah{confirmed} \ne \Lambda \land \Vah{predicted} \ne \Lambda$.
\end{tabular}
\end{center}
These will be consistent by
Observations~\ref{o:confirmed-AHFA-consistent}
and~\ref{o:predicted-AHFA-consistent}.

\subsubsection{Reduction consistency without Leo}
\label{s:reduction-consistent}

\begin{sloppypar}
We next show correctness for Marpa's reduction operation,
in the case where there is no Leo reduction.
There will be two cause EIM's, \Veim{predecessor}
and \Veim{component}.
\Veim{predecessor} will be correct by the outer induction
hypothesis
and \Veim{component}
will be correct by the inner induction hypothesis.
From \Veim{component}, we will find zero or more transition
symbols, \Vsym{lhs}.
From this point,  the argument is very similar to
that for the case of the scanning operation.
We know from the pseudocode for the scanning operation
that at most two EIM's will be added.
Where
\begin{gather*}
\Vah{confirmed} = \GOTO(\Vah{predecessor}, \Vsym{lhs}) \\
\text{and $\Vah{predicted} = \GOTO(\Vah{confirmed}, \epsilon)$,}
\end{gather*}
we will add
\begin{center}
\begin{tabular}{ll}
$[\Vah{confirmed}, \Vloc{i}]$ & if $\Vah{confirmed} \ne \Lambda$, and \\
$[\Vah{predicted}, 0]$ & if
$\Vah{confirmed} \ne \Lambda \land \Vah{predicted} \ne \Lambda$.
\end{tabular}
\end{center}
These will be consistent by
Observations~\ref{o:confirmed-AHFA-consistent}
and~\ref{o:predicted-AHFA-consistent}.
\end{sloppypar}

\subsubsection{Leo reduction consistency}
\label{s:leo-consistent}

\begin{sloppypar}
We now show correctness for Marpa's reduction operation,
in the case where there is a Leo reduction.
If there is a Leo reduction, it is signaled by the
presence of \Vlim{predecessor},
\begin{equation*}
\Vlim{predecessor} = [ \Vah{top}, \Vsym{lhs}, \Vorig{top} ]
\end{equation*}
in the Earley set where we would look
for the \Veim{predecessor}.
We treat 
the logic to create \Vlim{predecessor} as a matter of memoization
of the previous Earley sets,
and its correctness follows from
the outer induction hypothesis.
As the result of a Leo reduction,
\Leo{} will add
$[\Vdr{top}, \Vorig{top}]$
to \EVtable{\Leo}{j}.
Because the \Marpa{} LIM is correct,
Observations \ref{o:confirmed-AHFA-consistent}
and \ref{o:confirmed-AHFA-complete}
and Theorem \ref{t:leo-singleton},
we know that \Vah{top} is the singleton set
$\set{ \Vdr{top} }$
From the \Marpa{} pseudocode, we see
that, as the result of the Leo reduction,
\Marpa{} will add
$[\ah{top}, \Vorig{top}]$
to \EVtable{\Marpa}{j}.
Consistency follows immediately.
\end{sloppypar}

\subsubsection{Marpa's Earley sets are consistent}
\label{s:sets-consistent}

Sections
\ref{s:scan-consistent},
\ref{s:reduction-consistent}
and
\ref{s:leo-consistent}
show the cases for the step of the inner induction,
which shows the induction.
It was the purpose of the inner induction to show
that consistency of \EVtable{\Marpa}{i} is invariant
under Marpa's operations.

\subsubsection{Marpa Earley set completeness}

It remains to show that,
when Marpa's operations are run as described
in the pseudocode of Section \ref{s:pseudocode},
that 
\EVtable{\Marpa}{i} is complete.
To do this,
we need to show that
at least one EIM in \EVtable{\Marpa}{i}
corresponds to every EIMT in 
\EVtable{\Leo}{i}.
We will proceed by cases,
where the cases are \Leo{} operations.
For every operation which \Leo{} would perform,
we need to show that
\Marpa{} performs an operation which
produces a corresponding Earley item.
Here is how we divide the operations of \Leo{} into cases:
\begin{itemize}
\item traditional \Earley{} scanning operations;
\item traditional \Earley{} reductions;
\item Leo reductions;
\item traditional \Earley{} predictions;
\end{itemize}

\subsubsection{Scanning completeness}
\label{s:scan-complete}

\begin{sloppypar}
For scanning, the Marpa pseudocode shows
that, a scan is attempted for every
pair \Veim{predecessor}, \Vsym{token},
where \Veim{predecessor} is an EIM in the previous
Earley set,
and \Vsym{token} is the token scanned at \Vloc{i}.
(The pseudocode actually finds
\Veim{predecessor} in a set
returned by $\mymathop{transitions}()$.
This is a memoization for efficiency
and we will ignore it.)
\end{sloppypar}

By the preliminary definitions, we know that \Vsym{token}
is the same in both \Earley{} and \Leo.
By the outer induction hypothesis we know that,
for every traditional Earley item in the previous
Earley set,
there is at least one corresponding Marpa Earley item.
We see from the pseudocode, therefore,
that \Marpa{} performs its scan operation on a complete set
of correct operands.
Comparing the Marpa pseudocode (section \ref{p:scan-op}), 
with the Earley scanning operation (section \ref{d:scan})
and using
Observations~\ref{o:confirmed-AHFA-complete}
and \ref{o:predicted-AHFA-complete},
we see that a Earley item will be added to
\EVtable{\Marpa}{i} corresponding to every scanned Earley item
of \EVtable{\Leo}{i}.
We also see that the \Marpa{} scanning operation will
add to \EVtable{\Marpa}{i}
an Earley item for
every prediction that results from
a scanned Earley item in \EVtable{\Leo}{i}.

\subsubsection{Earley reduction completeness}
\label{s:reduction-complete}

For reduction,
we first examine Earley reduction,
under the assumption that there is
no Leo transition.
The Marpa pseudocode shows that the Earley items
\EVtable{\Marpa}{i}
are traversed in a single pass for reduction.
currently being examined by the reduction operation.

To show that these predecessors are paired
with a complete and correct sets of component Earley items,
we stipulate the Earley set is an ordered set,
and that new Earley items are added at the end.
The number of Earley items is at most $\Vsize{ah} \times \var{i}$,
where \Vsize{ah} is the number of AHFA states,
so a traversal of them must terminate.

Consider, for the purposes of an inner induction,
the reductions of \Leo{} to occur in generations.
Let the scanned Earley items be generation 0.
An EIMT produced by a reduction is generation $\var{n} + 1$
if its component Earley item was generation was \var{n}.
Predicted Earley items do not need to be assigned generations.
In Marpa grammars they can never contain completions,
and therefore can never act as the component of a reduction.

The induction hypothesis for the inner induction
is that for some \var{n},
the Earley items of \EVtable{\Marpa}{i} for generations 0 through \var{n}
are complete and consistent.

From Section \ref{s:sets-consistent},
we know that all Earley items in Marpa's sets are consistent.
In Section \ref{s:scan-complete},
we showed that generation 0 is complete --
its contains Earley items
corresponding to all of the generation 0 EIMT's of \Leo.
This is the basis of the inner induction.

Since we stipulated that \Marpa{} adds Earley items
at the end of each set,
we know that they occur in generation order.
Therefore, \Marpa{} while traversing \EVtable{\Marpa}{i}
can rely
on the inner induction hypothesis for 
the completeness of Earley items in
in generation \var{n}.
when creating Earley items of generation $\var{n}+1$.

Let
\begin{equation*}
\Veim{working} = [\Vah{working}, \Vorig{working}]
\end{equation*}
be the Earley item in \EVtable{\Marpa}{i}
currently being examined by the reduction operation.
From the pseudocode, we see
that reductions are attempted for every
pair \Veim{predecessor}, \Veim{working}.
(Again, $\mymathop{transitions}()$ is ignored
as a memoization.)
By the outer induction hypothesis we know that,
for every traditional Earley item in the previous
Earley set,
there is at least one corresponding Marpa Earley item.
We see from the pseudocode, therefore,
that for each \Veim{working}
that \Marpa{} performs its reduction operation on a complete set
of correct predecessors.

Comparing the Marpa pseudocode (Section \ref{p:reduction-op})
with the Earley reduction operation (Section \ref{d:reduction})
and using
Observations~\ref{o:confirmed-AHFA-complete}
and \ref{o:predicted-AHFA-complete},
we see that a reduced Earley item of
generation $\var{n}+1$
will be added to
\EVtable{\Marpa}{i} corresponding to every reduced Earley item
of \EVtable{\Leo}{i},
as well as one corresponding
to every prediction that results from
a reduced Earley item
of generation $\var{n}+1$ in \EVtable{\Leo}{i}.
This completes the induction step,
and the inner induction,
and shows the case of reduction completeness.

\subsubsection{Leo reduction complete}
\label{s:leo-complete}

We now show completeness for Marpa's reduction operation,
in the case where there is a Leo reduction.
In Section \ref{s:leo-consistent},
we found that where \Leo{} would create
the EIMT $[\Vdr{top}, \Vorig{top}]$,
Marpa adds 
$[\Vah{top}, \Vorig{top}]$
such that $\Vdr{top} \in \Vah{top}$.
Since \Vdr{top} is a completed rule,
there are no predictions.
This shows the case immediately,
by the definition of completeness.

\subsubsection{Prediction completeness}
\label{s:prediction-complete}

Predictions result only from items in the same Earley set.
In the sections for the other cases of Leo operations,
we showed that,
for every prediction that would result
from an item added to \EVtable{\Leo}{i},
a corresponding prediction
was added to \EVtable{\Marpa}{i}.

\subsubsection{Finishing the proof}
\begin{sloppypar}
Having shown the cases in Sections
\ref{s:scan-complete},
\ref{s:reduction-complete},
\ref{s:leo-complete}, and
\ref{s:prediction-complete},
we have shown that Earley set
\EVtable{\Marpa}{i} is complete.
In section \ref{s:sets-consistent}
we showed that \EVtable{\Marpa}{i} is consistent.
It follows that \EVtable{\Marpa}{i} is correct,
which is the step of the outer induction.
Having shown its step, we have the outer induction,
and the theorem.
\qedsymbol
\end{sloppypar}

\subsection{Marpa is correct}

We are now is a position to show that Marpa is correct.
\begin{theorem}
\begin{equation*}
\myL{\Marpa,\Vg} = \myL{\Vg}
\end{equation*}
\end{theorem}

\begin{proof}
From Theorem \ref{t:table-correct},
we know that
\begin{equation*}
[\Vdr{accept},0] \in \EVtable{\Leo}{\Vsize{w}}
\end{equation*}
if and only there is a
\begin{equation*}
[\Vah{accept},0] \in \EVtable{\Marpa}{\Vsize{w}}
\end{equation*}
such that $\Vdr{accept} \in \Vah{accept}$.
From the acceptance criteria in the \Leo{} definitions
and the \Marpa{} pseudocode,
it follows that
\begin{equation*}
\myL{\Marpa,\Vg} = \myL{\Leo,\Vg}.
\end{equation*}
By Theorem 4.1 in \cite{Leo1991}, we know that
\begin{equation*}
\myL{\Leo,\Vg} = \myL{\Vg}.
\end{equation*}
The theorem follows from
the previous two equalities.
\end{proof}

\section{Marpa recognizer complexity}
\label{s:complexity}

\subsection{Nulling symbols}
For the complexity proofs,
we consider only Marpa grammars without nulling
symbols.
When we examined correctness,
we showed that this rewrite
is without loss of generality
\ref{s:nulling}.
For complexity we must also show that
the rewrite and its reversal can be done
in amortized \Oc{} time and space
per Earley item.

One way to show the required bound is
to charge
the time and space involved in rewriting Earley
items with nulling symbols directly to
the Earley item to which they are rewritten
and from which they can be restored.
Since
the number of occurrences of nulling symbols in any rule
or in any AHFA state is a constant depending on the grammar,
the time and space involved are also constant,
and our bound follows.

\subsection{Complexity of each Earley item}

\begin{theorem}\label{t:O1-per-eim}
All time in \Marpa{} can be allocated
to the Earley items,
and in such a way that each attempt to
add an Earley item
requires \Oc{} time and space.
\end{theorem}

\begin{proof}
The theorem follows from the observations
in Section \ref{s:pseudocode}.
\end{proof}

\subsection{Duplicate dotted rules}

The same complexity results apply to \Marpa as to \Leo,
and the proofs are very similar.
\Leo's complexity results\cite{Leo1991}
are based on charging
resource to Earley items,
as were the results
in Earley's paper\cite{Earley1970}.
But both assume that there is one dotted rule
per Earley item,
which is not the case with \Marpa.

\Marpa's Earley items group dotted rules into AHFA
states, but this is not a partitioning in the strict
sense -- dotted rules can fall into more than one AHFA
state.
This is an optimization,
in that it allows dotted rules which often occur
together to be grouped together aggressively.
But it opens up the theoretical possibility
that, in cases where \Earley and \Leo disposed
of a dotted rule once and for all,
\Marpa might have to deal with it multiple times.

From an efficiency perspective,
\Marpa's duplicate rules
are by all the evidence, a plus.
From a worst-case complexity point of view,
they can be shown to be harmless,
but the price of doing so is building the
theoretical apparatus of this section.

\begin{theorem}\label{t:marpa-O-leo}
For a parse of \Vg{} and \Vw,
let \Rtablesize{\Leo} be the
number of traditional Earley items,
and \Rtablesize{\Marpa} be the
number of Marpa Earley items.
Then,
\begin{equation*}
    \Rtablesize{\Marpa} \le \var{c} \times \Rtablesize{\Leo},
\end{equation*}
where \var{c} is a constant which depends on the grammar.
\end{theorem}

\begin{proof}
We know from Theorem \ref{t:table-correct}
that every Marpa Earley item corresponds to one of
\Leo's traditional Earley items.
If an EIM corresponds to an EIMT,
the AHFA state of the EIM contains the
EIMT's dotted rule,
while their origins are identical.
The worst case is that a dotted rule appears
in every AHFA state,
so that
the number of Marpa items corresponding to a single
traditional Earley item cannot be greater
than $\size{\Vfa}$.
Therefore,
\begin{equation*}
    \Rtablesize{\Marpa} \le \size{\Vfa} \times \Rtablesize{\Leo},
\end{equation*}
\end{proof}

\begin{theorem}\label{t:tries-O-eims}
Let \Rtablesize{\Marpa} be the
number of Earley items actually
added to the tables,
and \var{c} be a constant.
For an unambiguous grammar,
the number of attempts to add
Earley items will be less than or equal to
$ \var{c} \times \Rtablesize{\Marpa} $
\end{theorem}

\begin{proof}
Earley\cite{Earley1970} shows that,
for unambiguous grammars,
every attempt to add
an Earley item will actually add one.
In other words, there will be no attempts to
add duplicate Earley items.
Earley's proof shows that for each attempt
to add a duplicate,
the causation must be different --
that the EIMT's causing the attempt
differ in either their dotted
rules or their origin.
Multiple causations for an Earley item,
would mean multiple derivations
for the sentential form that it represents,
which in turn would mean that
the grammar is ambiguous,
contrary to assumption.

In \Marpa, there is an slight complication,
because of which we must prove a result which is weaker,
but still sufficient to produce the same complexity results.
In \Marpa, a dotted rule can occur in more than one AHFA
state,
so that it is possible that two attempts to add a EIM
will have identical causation.
This means that attempts to add duplicate EIM's in
\Marpa{} are consistent with an unambiguous grammar.

We now proceed with our proof, stating
our argument more formally.
Let \var{initial-tries} be the number of attempts to add the initial item to
the Earley sets.
For Earley set 0, it is clear from the pseudocode
that there will be no attempts to add duplicate EIM's:
\begin{equation*}
\var{initial-tries} = \size{\Vtable{0}}.
\end{equation*}

Let \var{leo-tries} be the number of attempted Leo reductions in
Earley set \Vloc{j}.
For Leo reduction,
we note that by its definition,
duplicate attempts at Leo reduction cannot occur.
Let \var{max-AHFA} be the maximum number of 
dotted rules in any AHFA state.
Clearly there will be at most one Leo reduction for
each each dotted rule in the current Earley set,
\Vloc{j}.
every actual Earley
\begin{equation*}
\var{leo-tries} \le \var{max-AHFA} \times \size{\Vtable{j}}
\end{equation*}

Let \var{scan-tries} be the number of attempted Leo reductions in
Earley set \Vloc{j}.
Marpa attempts a scan operation,
in the worst case,
once for every EIM in the Earley set \Eloc{\var{j} \subtract 1}.
Therefore, the number of attempts
to add scans
scans must be less than equal to \size{\Vtable{j-1}},
the number
of actual Earley items at \Eloc{\var{j} \subtract 1}.
\begin{equation*}
\var{scan-tries} \le \size{\Vtable{j-1}}
\end{equation*}

Let \var{predict-tries} be the number of attempted predictions in
Earley set \Vloc{j}.
\Marpa{} includes prediction
in its scan and reduction operations,
and the number of attempts to add duplicate predicted EIM's
must be less than or equal
to the number of attempts
to add duplicate confirmed EIM's
in the scan and reduction operations.
So where
If \var{reduction-tries} is the number of the number of attempted Earley reductions
in Earley set \Vloc{j}, then
\begin{equation*}
\var{predict-tries} \le \var{reduction-tries} + \var{scan-tries}
\end{equation*}

The final and most complicated case is Earley reduction.
\Marpa{} attempts to add a reduced EIM
once for every each pair of LHS of every component
in its current Earley,
with a predecessor EIM in the Earley set
which is the origin of the component.
Here the memoization of $\mymathop{transitions}$
plays a role.
By $\mymathop{transitions}$,
Marpa makes a reduction attempt for
only those predecessors whose AHFA state
contains a dotted rule with a postdot symbol
that matches the transition symbol.
Call this number \Vsize{matching}.

The number of such attempts will be less than
\begin{equation*}
\var{max-AHFA} \times \Vsize{matching} \times \size{\Vtable{j}}
\end{equation*}
where \var{max-AHFA} is the maximum number of completed rules in
an AHFA state,
\Vloc{i} is the origin of the component EIM,
and $\size{\Vtable{j}}$ is the number of Earley items
in the Earley set being built.

Since by assumption
\Vg{} is unambiguous,
we know that all attempts to add duplicate
EIM's will be for predecessors
which have the same dotted rule.
(If this were not the case,
the two dotted rules would produce different
derivations, contrary to the assumption that
the grammar is ambiguous.)
As a worst case, assume that a dotted rule
appears in every AHFA state.
Then $\var{matching} \le \size{\Vfa}$.
From the forgoing, we know that 
\begin{equation*}
\var{reduction-tries} \le \var{max-AHFA} \times \size{\Vfa} \times \size{\Vtable{j}}
\end{equation*}

\begin{sloppypar}
The arithmetic for
summing \var{scan-tries},
\var{leo-tries},
\var{predict-tries}
and
\var{reduction-tries}
over the Earley sets,
then adding
\var{initial-tries}
is tedious.
But since \var{max-AHFA} and \size{\Vfa} are both constants
which depend only on \Vg,
it is clear that there is a constant \var{c}
such that
\begin{equation*}
\var{tries} \le \var{c} \times \Rtablesize{\Marpa}
\end{equation*}
where \var{c} is a constant which depends on \Vg.
\end{sloppypar}
\end{proof}

\subsection{The complexity results}
We are now in a position to show
specific time and space complexity results.
For these we follow the tradition by referring
letting \var{n} be
Vsize{\Vw},
the length of the input.

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in $\order{n}$ time and space.
\end{theorem}

\begin{proof}
By Theorem 4.6 in \cite[p. 173]{Leo1991},
the number of traditional Earley items produced by
\Leo when parsing input \Vw{} with an LR-regular grammar \Vg{} is
\begin{equation*}
\order{\Vsize{\Vw}} = \order{\var{n}}.
\end{equation*}
By Theorem \ref{t:marpa-O-leo},
the number of EIM's \Marpa{} will require for the same parse
is, worst case, $\var{c} \times \order{\var{n}}$,
where \var{c} is a constant which depends on \Vg.
LR-regular grammar are unambiguous, so that
by Theorem \ref{t:tries-O-eims},
the number of attempts that \Marpa{} will make to add
EIM's is
\begin{equation*}
\var{d} \times \var{c} \times \order{\var{n}} = \order{\var{n}},
\end{equation*}
where \var{d} is a constant which depends on \Vg.
Therefore,
by Theorem \ref{t:O1-per-eim},
the time and space complexity of \Marpa{} for LR-regular
grammars is $\order{\var{n}}$.
\end{proof}

\begin{theorem}
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time and space.
\end{theorem}

\begin{proof}
EIM's have the form $[\Vah{x}, \Vorig{x}]$.
\Vorig{x} is the origin of the EIM,
which in Marpa cannot be after the current
Earley set \Vloc{i},
so that
\begin{equation*}
0 \le \Vorig{x} \le \Vloc{i}.
\end{equation*}
Also, the possibilities for \Vah{x} are finite,
since the number AHFA states is a constant,
$\size{\Vfa}$,
which depends on \Vg.
Since duplicate EIM's are never added to an Earley set,
the maximum size of an Earley set is therefore
\begin{equation*}
\Vloc{i} \times \size{\Vfa}
\end{equation*}
Summing over the length of the input,
$\order{\Vsize{\Vw}} = \order{\var{n}}$,
the number of EIM's in all of \Marpa's Earley sets
is \order{\var{n}^2}.
By assumption, \Vg{} is unambiguous, so that
by Theorem \ref{t:tries-O-eims},
the number of attempts that \Marpa{} will make to add
EIM's is
\begin{equation*}
\var{c} \times \order{\var{n}^2} = \order{\var{n}^2},
\end{equation*}
where \var{c} is a constant which depends on \Vg.
Therefore,
by Theorem \ref{t:O1-per-eim},
the time and space complexity of \Marpa{} for LR-regular
grammars is \order{\var{n}^2}.
\end{proof}

\begin{theorem}
For any context-free grammar,
\Marpa{} runs in $\order{n^3}$ time.
\end{theorem}

\begin{proof}
Reexamining the proof of Theorem \ref{t:tries-O-eims},
we see that the only the bound which required
the assumption that \Vg{} was unambiguous
was \var{reduction-tries},
the count of the number of attempts to
add Earley reductions.
Let \var{other-tries}
be attempts to add EIM's other than
as the result of Earley reductions.
By Theorem \ref{t:tries-O-eims},
\begin{equation*}
\var{other-tries} \le \var{c} \times \Vloc{i}.
\end{equation*}

\begin{sloppypar}
Reconsidering \var{reduction-tries}
for the case of ambiguous grammars,
we see that in the worst case the memoization of 
$\mymathop{transitions}(\Vloc{i}, \Vsym{lhs})$
accomplishes nothing,
because every item in \Vloc{i} is returned.
Worst case, the count of EIM's in Earley set \Vloc{i}
is \order{\var{i}},
so that the number of attempts to add EIM reductions
in each Earley set \Vloc{i} is \order{\var{i}^2}.
Summing over \var{n} Earley sets,
the number of attempt to add EIM's as a result
of Earley reductions is \order{\var{n}^3}.
When \Vg{} can be ambiguous
the order of magnitude of all attempt to add EIM's
becomes
\begin{equation*}
 \sum\limits_{\var{i}=0}^{n}{\order{\var{i}^2}+ \var{c} \times \var{i}} = \order{\var{n}^3}
\end{equation*}
Therefore,
by Theorem \ref{t:O1-per-eim},
the time and space complexity of \Marpa{} for context-free
grammars is \order{\var{n}^3}.
\end{sloppypar}
\end{proof}

\section{Generalizing the grammar}
\label{s:generalization}

As implemented,
Marpa generalizes the idea of grammars
and input streams beyond that so far described
for \Vg{} and \Vw.
Because the differences were
minor from a theoretical point of view,
and their discussion has been deferred to avoid
cluttering the proofs.
This section redefines \Vg{} and \Vw,
to incorporate the
deferred generalizations.

First, Marpa's grammars are in effect 3-tuples:
$$(\Vsymset{alphabet}, rules, \Vsym{start})$$
\Vsymset{term} is omitted, because
Marpa allows a symbol to be both a terminal
and a LHS.
This expansion of the grammar definition
is made without loss of generalization,
or effect on the results.
\footnote{
Marpa has options which
cause the traditional restrictions to
be enforced,
in part or in whole.
For error detection and efficiency,
users may well prefer this.
}

Second, Marpa's input model is a generalization of
the traditional input stream model
used so far.
Marpa's input is a set of tokens,
$tokens$,
whose elements are triples of symbol,
start location and end location:
$$(\Vsym{t}, \Vloc{start}, \var{length})$$
such that
$$\var{length} \ge 1 \wedge \Vloc{start} \ge 0$$
The size of the input, \size{\Vw} is the maximum over
\var{tokens} of $\Vloc{start}+\var{length}$.

\begin{sloppypar}
Multiple tokens can start at a single location.
(This is how \Marpa{} supports ambiguous tokens.)
Tokens may have multiple lengths.
\Marpa's expanded concept of token lengths stretches
the current idea of parse location beyond its breaking point,
so that a new term for parse location is introduced,
the \dfn{earleme}.
Token length is measured in earlemes,
and the start and end location of a token is indicated in earlemes.
\end{sloppypar}

Just like standard parse locations, earlemes start at 0,
and run up to \size{\Vw}.
Unlike standard parse locations,
there is not necessarily a token {\emph at} any particular earleme.
(A token is considered to be ``at an earleme'' if it ends there,
so that there is never a token ``at'' earleme 0.)
In fact,
there may be earlemes at which no token either starts or ends,
although for the parse to succeed, such an earleme would have to be
``inside'' at least one token.
Here ``inside a token'' means after the token's start earleme
and before the token's end earleme.

In the Marpa input stream, tokens
may interweave and overlap freely,
but gaps are not allowed.
That is,
\begin{align*}
    & \forall \Vloc{i}, 0 \le \var{i} < \size{\Vw}, \\
   &  \quad \exists (\Vsym{t}, \Vloc{start}, \var{length}) \in \var{tokens}, \\
   & \quad\quad \var{start} \le \var{i} < \var{start}+\var{length}
\end{align*}

The traditional input stream can be seen as the special case of
a Marpa input stream where
for all \Vsym{x}, \Vsym{y}, \Vloc{x}, \Vloc{y},
\var{xlength}, \var{ylength}
if
\begin{center}
\begin{tabular}{rl}
(i)  & $[\Vsym{x}, \Vloc{x}, \var{xlength}] \in \var{tokens} $ \\
(ii) & $[\Vsym{y}, \Vloc{y}, \var{ylength}] \in \var{tokens}$ \\
\end{tabular}
\end{center}
then we have
\begin{center}
\begin{tabular}{rl}
(i) & $\var{xlength} = \var{ylength} = 1$ \\
(ii) & $\Vloc{x} = \Vloc{y} \implies \Vsym{x} = \Vsym{y}$ \\
\end{tabular}
\end{center}

The correctness results hold for Marpa input streams,
but to preserve the time complexity bounds,
two restrictions must be imposed.
Let the current Earley set be at \Vloc{i}.
Let the set of parse locations, $\mymathop{future}(\var{i})$
be such that $\Vloc{j} \in \var{future}$
if and only if
if there is a token
$[\Vsym{t}, \Vloc{start}, \var{length}]$
such that $\var{j} = \var{start} + \var{length}$
and $\var{start} \le \Vloc{i}$.
Assume that there is a constant \var{c} such
that these two restrictions are met:
\begin{itemize}
\item For all \Vloc{i},
the cardinality of $\mymathop{future}(\Vloc{i})$
is less than \var{c}.
\item The number of tokens which start at any one location
is less than \var{c}.
\end{itemize}
These two restrictions on Marpa input streams most
probably do not restrict their practical use.
And with them,
the complexity results for \Marpa{} stand.

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Irons}
Edgar~T.~Irons.
\newblock A syntax-directed compiler for ALGOL 60.
\newblock {\em Communications of the Association for Computing Machinery},
 4(1):51-55, Jan. 1961

\bibitem{Johnson}
Stephen~C. Johnson.
\newblock Yacc: Yet another compiler-compiler.
\newblock In {\em Unix Programmer's Manual Supplementary Documents 1}. 1986.

\bibitem{Marpa-HTML}
Jeffrey~Kegler, 2011: Marpa-HTML.
\newblock \url{http://search.cpan.org/dist/Marpa-HTML/}.

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2012: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock \url{http://search.cpan.org/dist/Marpa-XS/}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}

\clearpage
\tableofcontents

\end{document}
