% Copyright 2012 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{algorithm}
\usepackage{algpseudocode}

\newcommand{\comment}[1]{}
\newcommand{\sep}{\,\mid\,}
\newcommand{\mydot}{\raisebox{.15em}{\tiny $\,\bullet\,$}}
\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\order}[1]{{\mathcal O}(#1)}
\newcommand{\Oc}[1]{\ensuremath{\order{1}}}

% hyphens are often used in variable names,
% so that I need to ensure that subtraction is
% clearly designated in the typography
\newcommand{\subtract}{\,-\,}

\newcommand{\var}[1]{\ensuremath{\textbf{#1}}}

\newcommand{\cfg}{CFG}

\newcommand{\de}{\rightarrow}
\newcommand{\derivg}[1]{\mathrel{\mbox{$\:\Rightarrow\:$}}}
\newcommand{\derivrg}[2]{\mathrel{\mbox{$\:\stackrel{\!{#1}}%
        {\Rightarrow\!}\:$}}}

\newcommand{\set}[1]{{\lbrace #1 \rbrace} }
\newcommand{\ah}[1]{#1_{AH}}
\newcommand{\Vah}[1]{\ensuremath{\var{#1}_{AH}}}
\newcommand{\bool}[1]{\var{#1}_{BOOL}}
\newcommand{\Vbool}[1]{\ensuremath{\bool{#1}}}
\newcommand{\dr}[1]{#1_{DR}}
\newcommand{\Vdr}[1]{\ensuremath{\var{#1}_{DR}}}
\newcommand{\Vdrset}[1]{\ensuremath{\var{#1}_{\set{DR}}}}
\newcommand{\eim}[1]{#1_{EIM}}
\newcommand{\Veim}[1]{\ensuremath{\var{#1}_{EIM}}}
\newcommand{\Veimt}[1]{\ensuremath{\var{#1}_{EIMT}}}
\newcommand{\Veimx}[1]{\ensuremath{\var{#1}_{EIMX}}}
\newcommand{\Veimm}[1]{\ensuremath{\var{#1}_{EIMM}}}
\newcommand{\Veimset}[1]{\ensuremath{\var{#1}_{\set{EIM}}}}
\newcommand{\Veimtset}[1]{\ensuremath{\var{#1}_{\set{EIMT}}}}
\newcommand{\Veimxset}[1]{\ensuremath{\var{#1}_{\set{EIMX}}}}
\newcommand{\Veimmset}[1]{\ensuremath{\var{#1}_{\set{EIMM}}}}
\newcommand{\es}[1]{#1_{ES}}
\newcommand{\Ves}[1]{\ensuremath{\var{#1}_{ES}}}
\newcommand{\Vesi}[2]{\ensuremath{\var{#1}[#2]_{ES}}}
\newcommand{\VVes}[2]{\ensuremath{\var{#1}[\var{#2}]_{ES}}}
\newcommand{\Vlim}[1]{\ensuremath{\var{#1}_{LIM}}}
\newcommand{\Vlimx}[1]{\ensuremath{\var{#1}_{LIMX}}}
\newcommand{\loc}[1]{\var{#1}_{LOC}}
\newcommand{\Vloc}[1]{\ensuremath{\loc{#1}}}
\newcommand{\Vrule}[1]{\ensuremath{\var{#1}_{RULE}}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\var{#1}}}}
\newcommand{\sym}[1]{#1_{SYM}}
\newcommand{\Vsym}[1]{\ensuremath{\var{#1}_{SYM}}}
\newcommand{\symset}[1]{#1_{\lbrace SYM \rbrace} }
\newcommand{\Vsymset}[1]{\ensuremath{\var{#1}_{\set{SYM}}}}
\newcommand{\term}[1]{#1_{TERM}}
\newcommand{\token}[1]{#1_{TOK}}

\newcommand{\alg}[1]{\ensuremath{\textsc{#1}}}
\newcommand{\AH}{\ensuremath{\alg{AH}}}
\newcommand{\Earley}{\ensuremath{\alg{Earley}}}
\newcommand{\Leo}{\ensuremath{\alg{Leo}}}
\newcommand{\Marpa}{\ensuremath{\alg{Marpa}}}
\newcommand{\Emulator}{\ensuremath{\alg{Emulator}}}

\newcommand{\Vfa}{\var{fa}}
\newcommand{\Vg}{\var{g}}
\newcommand{\Vw}{\var{w}}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem*{definition}{Definition}

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\DeclareMathOperator{\scanned}{scanned}
\DeclareMathOperator{\Hyp}{Hyp}
\DeclareMathOperator{\GOTO}{GOTO}
\newcommand\myL[1]{\operatorname{L}(#1)}
\newcommand\tables[1]{\es{\var{table}[#1]}}
\newcommand{\tablesizen}[1]{\size{\var{table}(#1,\var{n})}}

% I want to use 'call' outside of pseudocode
\newcommand\call[2]{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}%

\hyphenation{ALGOL}

\begin{document}

\title{Marpa, a practical general parser: the recognizer}

\author{Jeffrey Kegler}
\thanks{
Copyright \copyright\ 2012 Jeffrey Kegler.
}
\thanks{
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\date{\today}

\begin{abstract}
This document reports describes the recognizer portion
of the Marpa algorithm.
The Marpa algorithm is a practical, and fully implemented,
algorithm for the recognition,
parsing and evaluation of context-free grammars.
Marpa's recognizer is based
on Earley's algorithm,
and merges the improvements made
to Earley's
by Joop Leo
with those of Aycock and Horspool.
The Marpa parse engine has its own added feature:
full knowledge of the state of the parse is available
as tokens are being scanned.
Advantageous for error detection,
this knowledge also
allows
"Ruby Slippers" parsing --
alteration of the input in reaction
to the parser's expectations.
\end{abstract}

\maketitle

\section{THIS IS AN EARLY DRAFT}

This a very early draft of this document,
and is largely unchecked.
The spirit and culture of the open source community
dictate that works of this kind and
at even this very early stage be made
available on-line in public repositories.
The reader should see the availability of this document
as compliance with that spirit and culture,
and not as in any way suggesting or
encouraging reliance on its contents.

\section{Introduction}

Despite the promise of general context-free parsing,
and the strong academic literature behind it,
it has never been incorporated into a tool
as widely available as yacc or
regular expressions.
The Marpa project was begun to end this neglect by
turning the best results from that literature
into a widely-available tool.
A stable version of this tool, Marpa::XS~\cite{Marpa-XS},
was uploaded to the CPAN Perl archive
on Solstice Day in 2011.

Sections
\ref{s:start-prelim} through \ref{s:end-prelim}
will outline notation and some of the
underlying concepts.
Section \ref{s:pseudocode} presents the pseudocode
for the algorithm Marpa.
Section
\ref{s:correctness}
contain a proof of correctness.
Section \ref{s:complexity} contains
the complexity results.
Finally, 
section \ref{s:generalization} generalizes
the definitions of grammar and input.

\section{Preliminaries}
\label{s:prel}
\label{s:start-prelim}

I assume familiarity with standard grammar notation
(pages 14-15 in Aho and Ullman~\cite{AU1972}).
In the past,
the type system required to support
a theory of parsing has been seen
as a challenge to the typographic
imagination,
often to become one to the eyesight.
This document will 
often use subscripts to indicate the commonly occurring types.
For example, $\sym{a}$ and $\sym{X}$ will be symbols.

Where $abced$ is a set of symbols,
let $abced^\ast$ be the set of all strings formed
from those symbols.
Let $abced^+$ be the subset of $abced^\ast$ that
contains all of its elements that are not of zero length.

For the purposes of this document consider,
without loss of generality,
a grammar $g$,
and a set of symbols, $alphabet$.
Call the language of \var{g}, $\myL{g}$,
where $\myL{g} \in alphabet^\ast$
Let its input be \var{w}, $\var{w} \in \var{alphabet}^\ast$.
Divide $alphabet$ into two disjoint sets,
$lh$ and $term$.

For the rewriting, designate a set of duples, $rules$,
where $\forall \Vrule{r} \sep \Vrule{r} \in rules$,
\Vrule{r} takes the form $\Vsym{lhs} \de \Vsymset{rhs}$,
where $\sym{lhs} \in term$ and 
$rhs \in alphabet^+$. 
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vsymset{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.
This definition follows \cite{AH2002},
which departs from tradition by disallowing an empty RHS.

The grammar $g$ can be defined as the 4-tuple
$$(\Vsymset{alphabet}, \Vsymset{term}, rules, \Vsym{start})$$.
Without loss of generality,
it is assumed that $g$ is "augmented",
so that \Vsym{start} is a dedicated start symbol
and
$\Vrule{start} = [ \Vsym{start} \de \Vsym{goal} ] $
is a dedicated start rule
such that for every rule, $\Vrule{x} = [ \Vsym{lhs} \de \Vsymset{rhs} ]$
\begin{center}
\begin{tabular}{ll}
1.\hspace{.5em} & 
$  \Vsym{start} = \Vsym{lhs} \implies \Vrule{x} = \Vrule{start} $ \\
2. &
$  \Vsym{start} \notin \Vsymset{rhs} $ \\
\end{tabular}
\end{center}

We have already noted
that no rules of \var{g}
have a zero-length RHS.
Further, no rule can be nullable
and all symbols must be either nulling or non-nullable --
no symbol can be a proper nullable.
These restrictions follow Aycock and Horspool~\cite{AH2002}.
The elimination of empty rules and proper nullables
is done by rewriting the grammar.
This can be done without loss of generality
and, in their paper~\cite{AH2002},
Aycock and Horspool
show how to do this
without effect on the time complexity
as a function of the input size.
Very importantly,
this rewrite is done in such a way that the semantics
of the original grammar can be efficiently reconstructed
at evaluation time.

Aycock and Horspool did allow a single empty start rule
to deal with null parses.
Marpa eliminates the need for empty rules in its grammars
by treating null parses and trivial grammars as special cases.
(Trivial grammars are those which recognize only the null string.)

In this document, \Earley{} will refer to the Earley's original
recognizer~\cite{Earley1970}.
\Leo{} will refer to Leo's revision of \Earley{}
as described in~\cite{Leo1991}.
\AH{} will refer to the Aycock and Horsool's revision
of \Earley{}
as described in~\cite{AH2002}.
Where $\alg{Recce}$ is a recognizer,
$\myL{\alg{Recce},g}$ will be the language accepted by $\alg{Recce}$
when parsing grammar $g$.

\section{The AHFA Finite Automaton}
\label{s:AHFA}

In this document a
"split LR(0) $\epsilon$-DFA"
a described by Aycock and Horspool~\cite{AH2002},
will be called an Aycock-Horspool Finite Automaton,
or AHFA.
A full description of how to derive an AHFA in theory
can be found in \cite{AH2002},
and examples of how to derive it in practice
can be found in the code for Marpa\cite{Marpa-RS,Marpa-XS}.
Here I will summarize those ideas behind AHFA's
that are central to Marpa.

Aycock and Horspool based their AHFA's
on a few observations.
\begin{itemize}
\item
In practice, Earley items with the same dotted rule
often appear in groups in the Earley sets,
where the entire shares the same origin.
\item
There was already in the literature a method
for associating groups of dotted rules that often appear together
when parsing.
This method was the LR(0) DFA used in the much-studied
LALR and LR parsers.
\item
The LR(0) items that are the components of LR(0)
states are, exactly, dotted rules.
\item
By taking into account symbols which derive the
null string, the LR(0) DFA could be turned into an
LR(0) $\epsilon$-DFA, which would be even more effective
at grouping dotted rules which occur together.
\end{itemize}

AHFA states are sets of dotted rules.
Aycock and Horspool realized that by changing Earley items
to track AHFA states, instead of individual dotted rules,
the size of Earley sets could be reduced,
and Earley's algorithm made faster in practice.
In short, then, AHFA states are a shorthand that Earley items
can use for groups of dotted rules that occur together frequently.
The original Earley items could be represented as $(\Vdr{r}, origin)$
duples, where \Vdr{r} is a dotted rule.
Aycock and Horspool modified their Earley items to be $(\ah{L}, origin)$
duples, where $\ah{L}$ is an AHFA state.

\begin{definition}
A dotted rule is {\bf Marpa-valid} if and only if
it does not have a nulling postdot symbol.
\end{definition}

AHFA states are of two kinds:
{\bf Discovered AHFA states}
contain the predicted start rule and
discovered rules.
{\bf Predicted AHFA states}
contain predicted rules
other than the start rule.
(In \cite{AH2002} discovered states are called the "kernel states",
and predicted states are called "non-kernel states".)

It is important to note that
an AHFA is not a partition of the dotted
rules --
a single dotted rule can occur
in more than one AHFA state.
This does not happen frequently,
but it does happens often enough,
even in practical grammars,
that the Marpa implementation has to provide for it.

What does not seem to happen in practical grammars
the size of \AH Earley set to grow larger
than that of one of Earley's original sets.
That is, it seems the AHFA is always a win,
at least for practical grammars.

Use of the AHFA states in EIM's can
be proved to be break-even for
time and space complexity purposes.
The next observation is useful for that purpose.

\section{The Leo algorithm}
\label{s:end-prelim}

\begin{theorem}\label{t:leo-singleton}
If the AHFA state of
a Marpa Earley item (EIMM) is the result of a
Leo collection,
then its AHFA state contains only one dotted rule.
\end{theorem}

\begin{proof}
Since the Earley item is the result of a Leo collection,
we know that its AHFA state contains a completed rule.
Call that completed rule, \Vdr{complete}.
Let \Vrule{c} be the rule of \Vdr{complete},
and \var{cp} its dot position.
$\var{cp} > 0$ because completions are never
predictions.

Suppose, for a reduction to absurdity,
that the AHFA state contains another dotted rule,
\Vdr{other},
$\Vdr{complete} \neq \Vdr{other}$.
Let \Vrule{o} be the rule of \Vdr{other},
and \var{op} its dot position.
AHFA construction never places a prediction in the same
AHFA state as a completion, so
\Vdr{other} is not a prediction.
Therefore, $\var{op} > 0$.

To create a contradiction, we first prove that
$\Vrule{c} \neq \Vrule{o}$,
then that 
$\Vrule{c} = \Vrule{o}$.
By the construction of an AHFA
state, both dotted rules resulted from the same series
of transitions.
But the same series of transitions over the
same rule would result in the same dot position,
$\var{cp} = \var{op}$
so that if $\Vrule{c} = \Vrule{o}$,
$\Vdr{complete} = \Vdr{other}$,
which is contrary to the assumption for the reduction.
Therefore, under the assumption for the reduction,
$\Vrule{c} \neq \Vrule{o}$.

Next we show that, under the assumption for the reduction,
that $\Vrule{c} = \Vrule{o}$
follows from \Leo's uniqueness requirement.
Since both dotted rules are in the same EIMM
and neither is a prediction,
both must result from transitions,
and their transitions must have been from the same Earley set.
Since they are in the same AHFA state,
by the AHFA construction,
that transition must have been
over the same transition symbol.
\Leo requires that, for each predecessor Earley set and transition symbol,
that a Leo transition be from a single rule.
The assumption for the reduction is that \Vdr{complete}
and \Vdr{other} are in the same Leo collection state,
and in order for them to have obeyed the Leo uniqueness
requirement,
we have $\Vrule{c} = \Vrule{o}$.

We now have both
$\Vrule{c} = \Vrule{o}$
and 
$\Vrule{c} \neq \Vrule{o}$,
completing the reduction to absurdity.
When \Vdr{complete} is in a Leo collection EIMM,
it must be the only dotted rule in that EIMM.
\end{proof}

\section{The Marpa Recognizer}
\label{s:recce}
\label{s:pseudocode}

As mentioned, it is assumed that the reader
is familar with \Earley{}
and with the description of \AH{}
in \cite{AH2002}.
The comments to the \Marpa{} pseudocode
will focus on its differences
from those two algorithms.

With the pseudocode are observations
about its space
and time complexity.
In what follows,
we will show that all time and space resources
can be charged to attempts to add Earley items,
in a way that each attempt to add an Earley item
takes amortized \Oc{} resource.
Below, when we present the complexity proofs,
we will characterize
the orders of magnitude of actual Earley items
and of attempts to add Earley items,
as well as their relationship to each other.

To achieve the 
amortized \Oc{} time and space
per Earley item, we will charge for those
resources in
three ways.

\begin{itemize}
\item We can also charge \Oc{} time and space to the parse itself.
We handle the failure cases in this way.
\item We will charge resources to the Earley set.
As long as the time or space is \Oc,
the time for the Earley set can be
re-charged to an arbitrary member of the Earley set.
If the Earley set is empty, the parse will fail at this Earley set,
and the time can be charged to the parse itself.
\item We will charge resources to attempts to add Earley items.
\end{itemize}

When discussing each procedure, we will state whether
our analysis of time and space usage is inclusive, exclusive
or caller-included.
The exclusive time or space of a procedure is that
which it uses directly,
ignoring resource usage by called procedures.
Inclusive time or space of a procedure includes
resource usage by called procedures.
Caller-included time and space is that which was already
been allocated as an inclusive resource by the procedure's
caller.

\begin{algorithm}[h]
\caption{Marpa Top-level}\label{a:initial}
\begin{algorithmic}[1] 
\Procedure{Main}{}
\State \Call{Initial}{}
\For{ $\var{i}, 0 \le \var{i} \le \Vsize{w}$ }
\State \Comment At this point, $\tables{\var{x}}$ is complete, for $0 \le \var{x} < \var{i}$
\State \Call{Scan Pass}{$\var{i}, \var{w}[\var{i} \subtract 1]$}
\State reject if $\size{\tables{\var{i}}} = 0$
\State \Call{Collect Pass}{\var{i}}
\EndFor
\If{$[\Vah{accept}, 0] \in \tables{\Vsize{w}}$}
\State accept \var{w}
\Else
\State reject \var{w}
\EndIf
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\subsection{Top-level code}

Exclusive time and space for the loop over the Earley sets
is charged to the Earley sets,
and overhead is charged to the parse.
All these resource charges are obviously \Oc.

\subsection{Ruby Slippers parsing}
This top-level code contains a significant difference
from \AH{}.
\call{Scan Pass} and \call{Collect Pass} are separated.
so that when scanning of tokens which start at location
$i$ begins,
the Earley sets for all prior locations are complete.
This means that the scanning operation has available, in
the Earley sets,
full information about the current state of the parse,
including which tokens are acceptable during the scanning phase.

This allows highly
accurate and flexible error detection,
but that is just the beginning.
It can be known immediately if a token
will be rejected, and the input can changed in
the light of the parser's expectations.
This means that error detection,
in many parsers an often quite desperate last resort,
is recoverable and inexpensive,
allowing its use as a parsing technique.
Because this can be described as making the parser's
"wishes" come true, I have called this "Ruby Sippers
Parsing".

One use of the Ruby Slippers technique is to
parse with a very clean,
but oversimplified grammar,
programming the lexical analyzer to make up for the grammar's
short-comings on the fly.
As an example that the author has implemented an HTML parser\cite{Marpa-HTML},
based on an grammar that assumes that all end
and start tags are present.
Such an HTML grammar is too simple even to describe fully
standard-conformant HTML, but by programming the lexical
analyzer to supply start tags as requested by the parser,
the application can parse very liberal HTML
and will produce a parse even for highly defective HTML.

\begin{algorithm}[h]
\caption{Initialization}\label{a:initial}
\begin{algorithmic}[1] 
\Procedure{Initial}{}
\State \Call{Add EIM Pair}{$0, \ah{start}, 0$}
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\subsection{Initialization}
Inclusive time and space is \Oc{} and
and can be charged to the parse.

\begin{algorithm}[h]
\caption{Marpa Scan Pass}\label{a:scan}
\begin{algorithmic}[1] 
\Procedure{Scan Pass}{$\Vloc{i},\Vsym{a}$}
\For{each $\Veimm{predecessor} \in \var{transitions}(\es{(\var{i} \subtract 1}),\Vsym{a})$}
\State $[\Vah{from}, \Vloc{origin}] \gets \Veimm{predecessor}$
\State $\ah{to} \gets \GOTO(\Vah{from}, \Vsym{a})$
\State \Call{Add EIM Pair}{$\Vloc{i}, \Vah{to}, \Vloc{origin}$}
\EndFor
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\subsection{Scan Pass}

\var{transitions} is a set of tables, one per Earley set,
which is indexed by symbol.
Symbol indexing is \Oc, since the number of symbols
is a constant, but
for the operation $\var{transitions}(\Vloc{l}, \Vsym{s})$
to be constant, there must be a link directly to the Earley
set.
In the case of scanning,
the lookup is always in the previous Earley set,
and it is safe to expect this can be found
in \Oc{} time.

There is one scan pass per Earley set,
so that overhead can be charged to the Earley set $i$.
Inclusive time and space can be charged to the 
Earley item attempt.

\begin{algorithm}[h]
\caption{Collection pass}\label{a:collection-phase}
\begin{algorithmic}[1] 
\Procedure{Collection pass}{\Vloc{i}}
\For{each Earley item $\Veimm{work} \in \tables{i}$}
\State $[\Vah{work}, \Vloc{origin}] \gets \Veimm{work}$
\For{each lhs of a completed rule, $\Vsym{lhs}$}
\State \Call{Collect one LHS}{\Vloc{i}, \Vloc{origin}, \Vsym{lhs}}
\EndFor
\For{all postdot symbols in Earley set $i$, \Vsym{postdot}}
\If{postdot transition is unique by rule}
\State Set $var{transitions}(\Ves{i},\Vsym{postdot})$ to
\State \hspace\algorithmicindent to an LIMX
\Else
\State Set $var{transitions}(\Ves{i},\Vsym{postdot})$ to
\State \hspace\algorithmicindent to the set of EIMX's which have
\State \hspace\algorithmicindent with \Vsym{postdot} as their postdot symbol
\EndIf
\EndFor
\EndFor
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\subsection{Collection Pass}

The \var{transitions} table for an Earley set
is built at the end
of the collection pass.
This can be done in
a single pass over Earley set \var{i},
and the time and space required charged to the
Earley items being examined.

Of special note is that in
setting up the \var{transitions} table for the current Earley set,
the Leo items must be created.
For each Earley item, the space
and time to create its resulting Leo items is \Oc{} --
worst case the number of Leo items created will be \Vsize{alphabet}.

\begin{algorithm}[h]
\caption{Collect one LHS symbol}
\begin{algorithmic}[1] 
\Procedure{Collection one LHS}{\Vloc{i}, \Vloc{origin}, \Vsym{lhs}}
\Statex \Comment $pim$ is a "postdot item", either a LIMX or an EIMM
\For{each $pim \in \var{transitions}(\Vloc{origin},\Vsym{lhs})$}
\If{\var{pim} is a LIMX, \Vlimx{pim}}
\State $\Vlimx{pred} \gets \var{pim}$
\State Perform \Call{Leo collection operation}{\Vloc{i}, \Vlimx{pred}}
\Else
\State $\Veimx{pred} \gets \var{pim}$
\State Perform \Call{Earley collection operation}{\Vloc{i}, \Veimx{pred}, \Vsym{lhs}}
\EndIf
\EndFor
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\subsection{Collect one LHS}

In examining the logic for collecting a single LHS symbol,
we will first look at its call to \var{transitions}.
We need to show we can find
$\var{transitions}(\Ves{origin},\Vsym{lhs})$
in \Oc{} time.
We do this by noting
that the number of symbols is a constant
and assuming that \Veimm{x} has a link back
to its origin Earley set.
(In fact, as implemented, Marpa's
Earley items have such links.)

\begin{algorithm}[h]
\caption{Earley collection operation}\label{a:earley-collection-op}
\begin{algorithmic}[1] 
\Procedure{Earley collection operation}{\Vloc{i}, \Veimm{from}, \Vsym{trans}}
\State $[\Vah{from}, \Vloc{origin}] \gets \Veimm{from}$
\State $\ah{to} \gets \GOTO(\Vah{from}, \Vsym{trans})$
\State \Call{Add EIM Pair}{\Vloc{i}, \Vah{to}, \Vloc{origin}}
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\subsection{Earley Collection operation}

\begin{algorithm}[h]
\caption{Leo collection operation}\label{a:leo-collection-op}
\begin{algorithmic}[1] 
\Procedure{Leo collection operation}{\Vloc{i}, \Vlim{from}}
\State $[\Vah{from}, \Vsym{trans}, \Vloc{origin}] \gets \Vlim{from}$
\State $\ah{to} \gets \GOTO(\Vah{from}, \Vsym{trans})$
\State \Call{Add EIM Pair}{\Vloc{i}, \Vah{to}, \Vloc{origin}}
\EndProcedure
\end{algorithmic} 
\end{algorithm}

\subsection{Leo Collection operation}

\begin{algorithm}[h]
\caption{Add EIM Pair}\label{a:pair}
\begin{algorithmic}[1] 
\Procedure{Add EIM Pair}{$i_{LOC},\ah{collected},origin_{LOC}$}
\State $\ah{predicted} \gets \GOTO(\ah{collected}, \epsilon)$
\State $\eim{collected} \gets [\ah{to}, origin_{LOC}]$
\If{$\eim{collected}$ is new}
\State Add $\eim{collected}$ to $\tables{i}$
\EndIf
\If{$predicted_{AHFA} \neq \Lambda$}
\State $\eim{predicted} \gets [\ah{predicted}, i]$
\If{$eim{predicted}$ is new}
\State Add $\eim{predicted}$ to $\tables{i}$
\EndIf
\EndIf
\EndProcedure
\end{algorithmic} 
\end{algorithm} 

\subsection{Adding a pair of Earley items}

This operation adds a collected EIMM
item and, if it exists, the EIMM for
its null-transition.
Time and space are allocated to the newly created
Earley items.
Trivially, the space is \Oc.
Time is also obviously \Oc{},
with the exception of two operations:
checking that an Earley item is new,
and adding it to the Earley set.
That an Earley item can be added to the current
set is clear if the 
Earley set is seen as a linked
list, to the head of which the new Earley item is added.

Earley items are only added if they are new,
A data structure that allows this
to be checked in \Oc{} time is very
briefly outlined, but not named, in
\cite[Vol. 1, pages 326-327]{AU1972}.
In this document that data structure will be call "per-Earley set lists",
or PSL's.
PSL's will be explained in the next section.

\subsection{Per-set lists}

A PSL is kept in each Earley set.
When building a new Earley set, \Ves{j},
the PSL for every previous Earley set, \Ves{i},
keep a list of those Earley items in \Ves{j} which have \Ves{i}
as their origin.
Each PSL per-set list needs to only keep track of a maximum
number of possible Earley items.
For traditional Earley items that maximum is the number
of dotted rules.
For Marpa, that maximum is the number of AHFA states.
Either way, the maximum is a constant which depends on the grammar.

Clearing and rebuilding the PSL's every time
a new Earley set would take more than \Oc{},
but can be avoided.
The per-AHFA state entries can be "stamped",
with a link back to the Earley set
which was the current one when that PSL
entry was last added or updated.

Consider the case where Marpa is building \Ves{j}
and looking at the PSL for \Ves{i} to check whether
an Earley item \Veimm{x} is new,
where $\Veimm{x} = [ \Vah{x}, \Vloc{j} ]$.
Marpa checks the PSL's for \Ves{i}.
Call the Earley set "stamp" for that PSL,
\Ves{p}.
If $\Ves{p} = \Ves{j}$, then \Veimm{x} is not new,
and will not be added to the Earley set.

If the PSL entry is new, or $\Ves{p} \ne \Ves{j}$, then \Veimm{x} is new.
\Veimm{x} is added to the Earley set,
and the PSL's "stamp" is changed to \Ves{j}.
Marpa's implementation uses PSL's
for this purpose and for several others.

\section{Marpa recognizer correctness}
\label{s:correctness}

Marpa's correctness will shown based the correctness
proved for \Leo{} in~\cite{Leo1991}.
The proof will be by induction
on the operations of the two recognizers.
For this induction,
the operations will need to happen
in the same order.
This will be accomplished by constructing a third
recognizers, \alg{Emulator}, which will,
in effect, emulate \Marpa{} using \Leo{}.
of operations.

\subsection{Nulling symbols}
\label{s:correctness:nulling}

As already noted, Marpa grammars
do not contain empty rules or
properly nullable symbols,
and shown that this is without loss
of generality.
For the correctness proof in this section,
it is assumed that the grammar is further rewritten,
this time to eliminate nulling symbols.

This this additional rewrite is also
without loss of generality, can be seen
by assuming that a history
of the rewrite is kept,
and that the rewrite is reversed
after the parse.
Clearly, whether an input \Vw{} is acceptable
or will not depend on how often
a grammar \Vg{} interpolates nulling symbols into
its input stream.

\subsection{The emulator in Leo mode}

\Emulator{} is \Leo,
with modifications.
These modifications will be explained in
detail below,
but in summary they include tracking the
AHFA states,
and adding a concept of
"activation" to Leo items and Earley items.
Tracking activation status allows
\Emulator{} to separate the creation
of Leo and Earley items
from their availability to the parse logic.
\Marpa{} and \Leo{} differ in their sequence
of creation,
and tracking activation makes
it convenient to compare the Earley and Leo
items of one with the other,
which is essential for an induction on the sequence
of operations.

\Emulator{} runs in two "modes",
Leo and Marpa,
which differ in their activation logic.
The sections which follow next
will describe activation
as it happens in
\Emulator's Leo mode.
When it comes time to describe activation
in Marpa mode, the change will
be easy to describe,
but significant in its implication.

\subsection{Calculating the AHFA}

\Emulator, as part of its precomputations
on its grammar, \Vg,
will calculate \Vah, the AHFA.
It will also calculate $\GOTO: \var{ah},\var{alphabet} \mapsto \var{ah}$,
the transition function for \var{ah}.

\subsection{Extending the Earley items}

The Earley sets of \Emulator{} will contain extended
Earley items, EIMX's.
EIMX's can be viewed as
crosses (``X") between traditional Earley items (EIMT's)
and Marpa's Earley items (EIMM's).

Each EIMX will have the form
$$[\Vbool{active}, \Vah{state}, \Vdr{dr}, \var{origin}]$$
where
\Vah{state} is an AHFA state,
\Vdr{dr} is a dotted rule,
\var{origin} is the origin,
and \Vbool{active} is a boolean whose use will be
explained.

\subsection{Extending the Leo items}

Leo items (LIM's) will also be extended.
The extended Leo items will take the form
$$[\Vbool{active}, \Vah{state}, \Vdr{dr}, \Vsym{trans}, \var{origin}]$$
where 
$$[\Vdr{dr}, \Vsym{trans}, \var{origin}]$$ is the original
Leo item,
\Vah{state} is the state of the Leo collection item
that will be added when the Leo item is used,
and \Vbool{active} is a boolean whose use will be
explained.

\subsection{Leo-Emulator Congruence}

A set of EIMX's (\Veimxset{X}) and a set of EIMT's
(\Veimset{T})
are congruent 
$$\Veimtset{X} \cong \Veimtset{T}$$
if and only if
for all
Vdr{x}, \var{xorig}
\begin{gather*}
(\exists \Vah{x} \sep \Vah{x}, \Vdr{x}, \var{xorig} \in \Veimxset{X}) \\
\iff [\Vdr{x}, \var{xorig}] \in \Veimtset{T}
\end{gather*}

\subsection{Emulation order}

For normal operation of 
\Leo and \Marpa, the precise order of items in the Earley sets is not
important,
and the pseudocode for each leaves that order open.
To simplify the induction on operations in the correctness
proof, however, it is convenient to synchronize the operations
of \Emulator{} in Marpa mode,
with those of Marpa.
For this purpose, we treat the Earley sets as ordered sets,
which containing the Earley item in the order in which
they were added.

Call Emulation Symbol Order,
some total ordering of the symbols.
Emulation Symbol Order may be arbitrary but must stay fixed for the life
of the induction.

Call some total ordering of the dotted rules,
Emulation Dotted Rule Order.
This ordering must place completed rules in Emulation Symbol Order by
LHS.
Otherwise, Emulation Dotted Rule Order
may be arbitrary except that it
must stay fixed for the life
of the induction.

We refine the order of operations as follows:
Both \Emulator{} and \Marpa{},
when scanning, scan the tokens in
Emulation Symbol Order.
Both \Marpa{},
in considering the left hand sides of completed rules
during collection,
does so in Emulation Symbol Order.
Both \Emulator{} and \Marpa{},
when creating Earley set 0,
create it in order by Emulation Dotted Rule
Order within AHFA state.
Finally, \Emulator{},
when adding the EIMX's for an AHFA state,
does so in Emulator Dotted Rule Order.
Together,
Emulator Symbol Order
and Emulator Dotted Rule Order are
referred to as Emulator Order.

For Earley sets after Earley set 0,
we require that the order of Earley items follow the
order in which they were added.
This definition ensures that,
that all EIMX's belonging
to the same AHFA state occur together
in an Earley set.
It is possible for the \Emulator{} to attempt
to add EIMX's belonging to a 
single AHFA state at several points while building
an Earley set, but when the first EIMX of an AHFA
state is added, all its co-occurring EIMX's are added
as well,
so that later attempts to add EIMX's in that AHFA state
will be rejected as attempts to add duplicates.

\subsection{The emulation engine}

For scanning, collection and prediction,
\Emulator{} follows
the \Leo{} logic,
except that it uses EIMX's instead of EIMT's.
\Emulator{} runs through each Earley set repeatedly,
until no new EIMX's can be added.
EIMX's are never added if they are duplicates,
and the number of possible EIMX's for a given Earley set
is finite,
so that we know the loop will terminate.

\Veimx{a} and \Veimx{b} are considered duplicates if
\begin{gather*}
\Veimx{a} = [\Vbool{a}, \Vah{a}, \Vdr{a}, \Vloc{a}] \\
\land \Veimx{b} = [\Vbool{b}, \Vah{a}, \Vdr{a}, \Vloc{a}]
\end{gather*}
Two EIMX's which differs only in their active boolean
are considered identical.
It is quite possible that two EIMX's may differ when
their corresponding EIMT's are duplicates --
EIMX's are not duplicates
if they differ in their AHFA states.

\Leo{}, when traversing its Earley sets,
expects EIMT's.
\Emulator, during its processing of its Earley sets,
will encounter EIMX's.
Let the EIMX that \Emulator{} encounters be
$$\Veimx{cause} =
[\Vbool{active}, \Vah{state}, \Vdr{dr}, \var{origin}]$$
\Veimx{cause} may be enountered
as the predecessor in a scan operation;
as the predecessor or component in a collection operation;
or as the predecessor in a prediction operation.
In each of these cases,
\Emulator{} first checks the
\var{active} boolean of the EIMX.
If it is false, the Emulator ignores the EIMX
as if it did not exist.
If \Vbool{active} is true, treats \Veimx{cause} as
if it were the following EIMT:
$$[\Vdr{dr}, \var{origin}]$$
Informally, it can be said that
\Emulator{} strips its extended information from an active EIMX,
and ignores inactive EIMX's.

Similarly, 
while \Leo{} uses LIM's,
during \Emulator{} will encounter LIMX's.
LIMX's are encountered as predecessors
during Leo collection.
Let an encountered LIMX be
$$\Vlimx{predecessor} = [\Vbool{active}, \Vah{state}, \Vdr{dr}, \Vsym{trans}, \var{origin}]$$
\Emulator{} first checks the
\var{active} boolean of the LIMX.
If it is false, the Emulator ignores the LIMX
as if it did not exist.
If \Vbool{active} is true, treats \Vlimx{predecessor} as
if it were the following LIM:
$$[\Vdr{dr}, \Vsym{trans}, \var{origin}]$$
Informally, for use in Leo collection,
\Emulator{} strips its extended information from an active LIMX,
and ignores inactive LIMX's.

Once it has found the causes for an Earley operation,
\Emulator{} proceeds to determine
\Vah{new} and \Vah{predict},
two AHFA states.
It does this as follows:
\begin{itemize}
\item
If it is scanning \Vsym{token},
$Vah{new} = \GOTO(\Vah{predecessor}, \Vsym{token})$,
where 
where \Vah{predecessor} is the AHFA state of the predecessor EIMX.
\item
If it is collecting, and Leo collection does not
apply, then
$Vah{new} = \GOTO(\Vah{predecessor}, \Vsym{lhs})$,
where \Vah{predecessor} is the AHFA state of the predecessor EIMX.
and \Vsym{lhs} is the LHS of the completed rule.
\item
If it is collecting, and Leo collection does apply,
$\Vah{new} = \Vah{leo}$, where \Vah{leo} is
the state in the extended Leo item.
Leo collection in the \Emulator{}
will be described below.
\Vah{predict} is set to $\GOTO(\Vah{predecessor}, \epsilon)$
\item For predictions, $\Vah{new} = \Lambda$.
Call \Vah{cause}, the AHFA state of the EIMX that caused
the prediction.
If \Vah{cause} is itself a prediction, $\Vah{predict} = \Vah{cause}$.
If \Vah{cause} is not a prediction, $\Vah{predict} = \GOTO(\Vah{cause}, \epsilon)$.
\end{itemize}

If $\Vah{new} \neq \Lambda$,
\Emulator{} produces
\Veimxset{collected}, which is the set of EIMX's
\begin{gather*}
\set{ [ \Vbool{new}, \Vah{new}, \Vdr{dr}, \var{origin} ] \sep \Vdr{dr} \in \Vah{new} }
\end{gather*}
where \var{origin} is the origin of \Veimx{cause}.
\Vbool{new} is \var{T} if and only if
\begin{center}
\begin{tabular}{rl}
(i) & $ \Vloc{i} = 0 $ \\
(ii) & $\forall \Vah{x}
    \sep [ \Vbool{T}, \Vah{x}, \Vdr{dr}, \var{origin} ] \notin \Vesi{E}{i}$ \\
\end{tabular}
\end{center}
Duplicate EIMX's in \Veimxset{collected} are ignored,
and the others are added to the current Earley set
in Emulator Dotted Rule Order.

Next, if $\Vah{predict} \neq \Lambda$,
\Emulator{} produces \Veimxset{predict}
$$\Veimxset{predict} =
{ [ \Vbool{new}, \Vah{predict}, \Vdr{dr}, \Vloc{i} ] \sep \Vdr{dr} \in \Vah{predict} }$$
where \Vloc{i} is the current Earley set and
\Vbool{new} is \var{T} if and only if
\begin{center}
\begin{tabular}{rl}
(i) & $ \Vloc{i} = 0 $, or \\
(ii) & the current operation is prediction
\end{tabular}
\end{center}

If any of the EIMX's in \Veimxset{collected} or 
\Veimxset{predict} duplicates an unactivated EIMX,
the EIMX currently in \Vesi{E}{i} is activated,
without disturbing its position in the ordered EIMX set.
An EIMX which duplicates one already in \Vesi{E}{i}
is ignored for all other purposes.
Non-duplicate EIMX's are added to \Vesi{E}{i},
first those in 
\Veimxset{collected} in Emulator Order,
then those in
\Veimxset{predict}, also in Emulator Order.

\subsection{Creating Leo items}

Unlike \Leo,
\Emulator{} compute the LIMX's on an eager basis,
immediately
after each Earley set is finished.
This is possible because there is enough information
in the Earley set to identify potential LIM's.

\subsection{Initialization in the Emulator}

It is convenient to treat \Emulator{}
initialization is handled as a special case.
To build Earley set 0, \Emulator{}
first adds the set
\begin{equation*}
\set{ [ \Vah{start}, \Vdr{dr}, 0 ] \sep \Vdr{dr} \in \Vah{start} }
\end{equation*}
in Dotted Rule Order.
It then examines \Vah{prediciton},
where $\Vah{prediction} = \GOTO(\Vah{cause}, \epsilon)$.
If $\Vah{prediction} \neq \Lambda$,
Emulator adds the set
\begin{equation}
\set{ [ \Vah{predicted}, \Vdr{dr}, 0 ] \sep \Vdr{dr} \in \Vah{predicted} }
\end{equation}
in Emulator Dotted Rule Order.

\subsection{Leo mode}

The emulator runs in modes.
The intent of Leo mode is to emulate \Leo.
\Emulator{} is "eager" in its creation of Earley items.
The idea behind the following rules is
that the active boolean tracks
whether \Leo{} would have created the EIMX.
\begin{itemize}
\item EIMX's in Earley set 0 are always
created with their active booleans
set to true, regardless of any other rules.
\item The EIMX of the base EIMX is set to true.
This is the case whether the base EIMX is being created,
or already exists.
\item If an EIMX already exists,
and it is not the base EIMX,
it active boolean is left as it was.
\item If an EIMX does not exist,
and is being created,
it is created with its active boolean set to false.
\item When an LIMX is created, its active boolean is set to false.
\item When an LIMX is used in Leo collection, its active boolean
is set to true.
\item
Once the active boolean of an EIMX or LIMX is true, its value
will never change.
\end{itemize}

\subsection{The emulator in Leo mode is correct}

We now show that,
for a given grammar \var{g} and input \var{w},
the Earley sets of \Emulator{} are always congruent
to those of \Leo.

\begin{theorem}\label{t:leo-congruence}
Let $\VVes{L}{i}$
be Earley set \Vloc{i} in \Leo,
and $\VVes{El}{i}$
be Earley set \Vloc{i} in \Emulator.
running in Leo mode.
\begin{equation*}
\forall \Vloc{i} \sep 0<\Vloc{i}<\Vsize{w} \implies
\Vesi{L}{i} \cong \Vesi{El}{i}
\end{equation*}
\end{theorem}

\begin{proof}
It is straightforward to confirm that
$$ \Vesi{L}{0} \cong \Vesi{E}{0}$$
We next consider
Earley operations in the sets after Earley set 0.
By construction,
the definition of \Emulator{} in \Leo{} mode is
an extension of the definition of \Leo.
The Earley operations of \Emulator
will produce Earley sets where the every EIMX
corresponds to an EIMT created by \Leo.
Congruence follows directly.
\end{proof}

\subsection{Emulator in Marpa mode}

Marpa mode is simply \Emulator{} with "eager" activation.
In Marpa mode,
all EIMX's and LIMX's are activated as soon as they are
created and remain activated throughout the life
of \Emulator.

\subsection{The emulator in Marpa mode is correct}

To prove correctness,
we show that the emulator in Marpa mode produces
the same Earley sets as the emulator in Leo mode.

\begin{lemma}
An emulator with eager activation of Leo items,
produces the same Earley sets as the emulator with
lazy activation of Leo items.
\end{lemma}

\begin{proof}
Inspection of the logic will show that the active boolean
of the LIMX
is useless bookkeeping, without effect on the Earley sets.
\end{proof}

An EIMX is correct in Earley set $i$
if and only if its corresponding EIMT would
also be in Earley set $i$.

\begin{lemma}\label{l:eimx-correct}
All EIMX's, whether active or not,
are correct.
\end{lemma}

\begin{proof}
The proof is by cases:
the base EIMX;
EIMX's occurring in the AHFA state as the base EIMX;
and EIMX's occurring in a predicted state.

\begin{itemize}
\item The base EIMX is correct because it results from
the \Leo{} logic, which is known to be correct\cite{Leo1991}.
\item The EIMX's occurring in the same AHFA state
as the base EIMX are correct, as shown in \cite{AH2002}.
\item The EIMX's occurring in a predicted AHFA state
as the base EIMX are correct, as shown in \cite{AH2002}.
\end{itemize}

\end{proof}

\begin{lemma}\label{l:lazy-complete}
Even with lazy activation of EIMX's,
all EIMX's in Earley set $i$
will be active once processing for Earley set $i$
is complete.
\end{lemma}

\begin{proof}
By Lemma \cite{l:eimx-correct},
all EIMX's created by the emulator engine are
correct.
Lazy activation simulates Leo and Leo has been
shown to include all the correct EIMT's
in every Earley set.
For every EIMT created by \Leo,
lazy activation activate all the corresponding
EIMX's.
Therefore lazy activation activates all the
EIMX's in any \Emulator{} Earley set.
\end{proof}

\begin{lemma}
An emulator with eager activation of Earley items,
produces the same Earley sets as the emulator with
lazy activation of Earley items.
\end{lemma}

\begin{proof}
The proof is by induction on the Earleys sets.
The two modes do not differ for Earley set 0.
We proceed to show that if eager and lazy activation
have produced identical Earleys sets 0 through $i - 1$,
then the two activation methods will also produce
identical Earleys sets at $i$.

By Lemma \cite{l:lazy-complete},
once processing for an Earley set is complete,
all of its EIMX's are activated,
whether activation is lazy or eager,
and activated EIMX's never change their activation status.
This means that to show that the Earley sets
are identical we need to show that each
operation creates the same EIMX's
under both modes.
It also means that
an operation will behaves
differently under lazy activation and
eager activation only if it
depends on an EIMX in the current Earley set.

We examine those cases where
creation of an Earley item in Earley set $i$,
depends on another Earley item in Earley set $i$.
For scanning, the predecessor EIMX must be
in the previous Earley set, so that
this will never be the case.
Scanning therefore produces the new
EIMX's
whether activation is lazy or eager.

For prediction, the predecessor EIMX will always
be in the current Earley set.
But the predecessor EIMX, when it created
and activated EIMX's for all the dotted rules in its
AHFA state pair, created and activated of its
predictions.
This means that all predictions will already
exist and be activated before any \Emulator
prediction operation.
There an Emulator prediction operation will
always create duplicate EIMX's,
and since duplicates are not added,
will not change the Earley set.

For collection,
we now show,
for eager activiation,
the basis of an induction on
the collection loop's visits to
the potential component Earley items in an Earley set.
We note from the forgoing that, under eager activation,
all non-collected Earley items exist and are active,
before the emulator's collection loop begins.
Scanned items already exist and are active because scanning
was done first and we have just shown it complete and correct.
Prediction will come afterwards, but we have just shown it
will not add new EIMX's or activate any old ones.

The \Leo{} pseudocode
(and therefore \Emulator) creates
collected EIMX's by repeatedly looping
through potential component EIMX's,
looking for predecessor's as it goes.
Because we are running in eager mode,
we can rely on all of these component EIMX's being already
active.
Marpa allows no nullable rules, so component
will be zero length,
and the EIMX's predecessors must be in previous Earley sets.
For for every potential component, its matching
predecessors will exist and be active when the component is visited.
The logic will always add the collected EIMX's.
That these are identical to those that would be added
under lazy activation can be seen by noting that once
the causes are identified and the active status checked,
both use the same logic.
This shows the induction step on the eager collection
loop, and the induction on eager collection.

Since prediction, scanning and collection create
the same EIMX's in the two modes,
and, and since in either mode, all are activated when
the Earley set is finished,
we have shown the induction step for the Earley sets.
This completes the induction on the Earley sets,
and the proof.
\end{proof}

\subsection{Marpa is correct}
\label{s:marpa:end-correctness}

A set of EIMX's (\Veimxset{X}) and a set of EIMM's
(\Veimset{M})
are congruent 
$$\Veimtset{X} \cong \Veimtset{M}$$
if and only if
for all
\Vah{x}, \var{xorig}
\begin{gather*}
\Vbool{x}, \Vah{x}, \Vdr{x}, \var{xorig} \in \Veimxset{X} \\
\iff [\Vah{x}, \var{xorig}] \in \Veimmset{M}
\end{gather*}


We now show that,
for a given grammar \var{g} and input \var{w},
the Earley sets of \Emulator{} are always congruent
to those of \Marpa.

\begin{theorem}\label{t:marpa-congruence}
Assume that \Emulator{} is in Marpa mode,
and that the \Marpa{} and
\Emulator{} operate using
Emulator Order.
Let
$\operatorname{E}[i]$
be Earley set $i$ in \Emulator.
and $\operatorname{M}[i]$
be Earley set $i$ in \Marpa,
\begin{equation*}
\forall i \sep 0<i<\size{w} \implies
\operatorname{E}[i] \cong \operatorname{M}[i]
\end{equation*}
\end{theorem}

\begin{proof}
The proof is by double induction.
The outer induction is on the Earley sets,
and the inner induction on "basic operations".

We define the "basic operations" by breaking
down the operations of \Emulator{} and \Marpa
into convenient pieces, excluding
operations which can be shown
to have no effect on the Earley sets.
A basic operation is
\begin{itemize}
\item
A scan for one pairing of predecessor and token.
\item
An Earley collection for one pairing of predecessor and LHS.
\item
An Leo collection for one pairing of predecessor and LHS.
\end{itemize}
Predictions are not among the basic operations
because, as shown previously,
they have no effect on the Earley sets in \Emulator's Marpa mode.
Predictions do not exist in \Marpa.

The hypothesis for the outer induction is that
$$\forall \var{i} \sep 0<\var{i}<\var{n}
\implies
\VVes{EM}{i} \cong \VVes{M}{i}$$
and that
$\VVes{EM}{i}$ is in Emulation Order.

It is straightforward to confirm,
as the basis of the outer induction,
that
$$ \Vesi{Em}{0} \cong \Vesi{M}{0}$$
and that $\Vesi{Em}{0}$ is in Emulation order.

The inner induction is done on the operations to build
the Earley sets for $\var{i}>0$.
The hypothesis for the inner induction is that,
up to this point,
the following three invariants hold:
\begin{center}
\begin{tabular}{l p{1in} p{3.5in}}
1. \hspace{1em} & Congruence &
$\Vesi{EM}{i} \cong \Vesi{M}{i}$ \\
2. & Item Order & \Vesi{EM}{i} is in Emulation Order \\
3. & Operation \mbox{Sequence} &
\Emulator{} in Marpa mode and \Marpa{} carry out the
same basic Earley operations.
\end{tabular}
\end{center}

As the basis of inner induction, we observe
that, trivially, the two empty Earley sets are congruent,
that an empty Earley set is in Emulation order.
To show that the next operations will be the same,
we observe that
that the first basic operation for both recognizers will be
the scan whose predecessor comes first in the 
the previous Earley set,
and that by the outer induction hypothesis,
both previous Earley sets will be in Emulation order.

For the step of the inner induction,
it is convenient to first show that
the Operation Sequence invariant holds.
If we assuming the induction hypothesis
for the inner induction,
we can see,
based on the Emulation Order,
the pseudocode for \Leo
and the pseudocode for \Marpa
that this sequence of basic operations will be
scan operations, then collection operations.
Scan operation will be in order by predecessor,
and collection operations in order by predecessor
within LHS.

By the outer induction hypothesis, 
Earley items of \Emulator{} and \Marpa{}
will be in the same order by AHFA state
for the predecessors.
Based on Emulation Symbol Order and the
inner induction hypothesis,
we know that the two recognizers
will process the collection operations
in the same order by LHS symbol.

The Item Order invariant is straightforward to show.
As long as the inner and outer induction hypotheses
are true,
by their definitions,
\Emulator{} and \Marpa{} add all Earley items in
Emulator Order.

To complete the step of the inner induction,
we now proceed to show that Congruence stays invariant
as new Earley items are added.
We do this by cases, where the bases are the basic operations.

For the cases of scanning and Earley collection,
we recall that \Marpa{} and \Emulator{}
both implement the transition logic of AHFA's,
as described in \cite{AH2002}.
We know from the inner and outer induction hypotheses
that they will find congruent predecessors
from congruence that the LHS's of completed rules will be the same,
and from the definition of the recognizer that the tokens will be
the same.
From this we can see that,
for every EIMM added by \Marpa,
as least one EIMX (the base EIMX) will be added
by \Emulator{} in Marpa mode,
and that the EIMX will have the same AHFA state
as the EIMM.
Therefore, the Earley items added by each scanning
or Earley item operation
will be congruent.
The union of congruent sets of Earley items is also congruent,
so that the Earley sets under construction remain congruent
as items from these operations are added.
This show the case of the Congruence invariant
for scanning and Earley collection operations,
as part of the inner induction step.

To complete the inner induction step,
it remains to show the case of the Congruence invariant
for the Leo collection operation.
By the definition of Leo collection, it
produces a single AHFA state,
call this \Vah{collected}.
By Theorem \ref{t:leo-singleton},
\Vah{collected} will contain one and only one dotted rule,
\Vdr{collected}.
\Vdr{collected} will be a collection.
Since the only rule in the \Vah{collected} is a collection,
\Vah{collected} will have no null-transition.
Therefore
a Leo collection operation
in \Marpa{}
will add only one EIMM, $[\Vah{collected}, \Vloc{origin}]$,
where \Vloc{origin} is from the LIMX.
\Emulator{} in Marpa mode will add only
those EIMX in the set
$${
[ \Vbool{active}, \Vah{collected}, \Vdr{d}, \Vloc{origin} ]
\sep \Vdr{d} \in \var{null}(\Vdr{collected}) }
$$
Clearly, congruence will hold with the addition to these
Earley items to their respective Earley sets.

The completes the inner induction step, the inner induction,
the outer induction step, the outer induction and the proof.
\end{proof}

We now show that \Marpa,
the language accepted by Marpa,
is $\myL{\var{g}}$ --
in other words, that \Marpa{} is correct.

\begin{theorem}\label{t:correctness}
$$\myL{\Marpa, g} = \myL{\var{g}}$$
\end{theorem}

\begin{proof}
Assume that \var{w} is a string of size $\size{w}$,
We will assume that
$$ \var{w} \in \myL{\var{g}}$$

From Leo's correctness\cite{Leo1991},
where \Vdr{accept} is the completed start rule,
$$ [\Vdr{accept}, 0] \in \Vesi{L}{\size{w}}$$
By Theorem \ref{t:leo-congruence},
$$
\exists \Vbool{b}, \Vah{x}
\sep [\Vbool{b}, \Vah{x}, \Vdr{accept}, 0] \in \Vesi{El}{\size{w}}
$$
Where \Vah{accept} be an AHFA state,
$\Vdr{accept} \in \Vah{accept}$,
so that by the definition of \Emulator, in at least one case
we have $\Vah{x} = \Vah{accept}$,
$$
\exists \Vbool{b} 
\sep [\Vbool{b}, \Vah{accept}, \Vdr{accept}, 0] \in \Vesi{El}{\size{w}}
$$
By Theorem \ref{t:emulator-equivalence},
$$
\exists \Vbool{b}
[\Vbool{b}, \Vah{accept}, \Vdr{accept}, 0] \in \Vesi{Em}{\size{w}}
$$
By Marpa-Emulator Congruence, Theorem \ref{t:marpa-congruence},
$$
[\Vah{accept}, 0] \in \Vesi{M}{\size{w}}
$$
By the definition of \Marpa,
$$
\var{w} \in \myL{\Marpa, \var{g}}
$$
Collapsing the forgoing chain of implication,
back to its original assumption,
$$ \var{w} \in \myL{\var{g}} \implies \var{w} \in \myL{\Marpa, \var{g}} $$

Reviewing the chain of implications,
will show that each was in fact a mutual implication,
so that
$$ \var{w} \in \myL{\var{g}} \iff \var{w} \in \myL{\Marpa, \var{g}} $$
and
since \var{w} was chosen without loss of generality, the theorem
follows.

\end{proof}

\section{Marpa recognizer complexity}
\label{s:complexity}

\subsection{Nulling symbols}
For the complexity proofs,
we consider only Marpa grammars without nulling
symbols.
When we examined correctness,
we showed that this rewrite
is without loss of generality
\ref{s:correctness:nulling}.
For complexity we must also show that
the rewrite and its reversal can be done
in amortized \Oc{} time and space
per Earley item.

One way to show the required bound is
to allocate
the time and space involved in rewriting Earley
items with nulling symbols directly to 
the Earley item to which they are rewritten
and from which they can be restored.
Since
the number of occurrences of nulling symbols in any rule
or in any AHFA state is a constant depending on the grammar,
the time and space involved are also constant,
and our bound follows.

\subsection{The order of Marpa's Earley items}

\begin{theorem}\label{t:eim-order}
Let \Vg{} be a grammar as defined in this
document,
and let \Vfa{} be the AHFA for this grammar.
Let \tablesizen{\Marpa} be the number of Earley items in
a \Marpa{} parse,
and \tablesizen{\Leo} the number of Earley items in a \Leo
parse.
$$ \order{\tablesizen{\Marpa}} = \order{\tablesizen{\Leo}} $$
\end{theorem}

\begin{proof}
Let \VVes{L}{i} be
an Earley set of \Leo.
Let \VVes{El}{i} be an
an Earley set of \Emulator{}
in Leo mode.
By Theorem \ref{t:leo-congruence},
we know that that for every EIML
$$[\Vdr{l}, \Vloc{origin}] \in \Vesi{L}{i}$$
there is at least one EIMX,
$$[\Vbool{b}, \Vah{el}, \Vdr{l}, \Vloc{origin}] \in \Vesi{El}{i}
$$
We also know that for every EIMX,
$$ [\Vbool{b}, \Vah{el}, \Vdr{l}, \Vloc{origin}] \in \Vesi{L}{i}
$$
that
$$[\Vdr{l}, \Vloc{origin}] \in \Vesi{L}{i}$$

From this we see that there is a total function
from
$\size{\Vesi{L}{i}}$ to a partition
of $\size{\Vesi{El}{i}}$,
such that $\Veimt{x} \mapsto \Veimxset{y}$,
where
\begin{align*}
&& \Veimt{x} = & [ \Vdr{x}, \Vloc{x} ] \\
& \land & \Veimx{y} = &
\set{ [ \Vbool{z}, \Vah{z}, \Vdr{x}, Vloc{x} \sep \Vdr{x} \in \Vah{z} ] }
\end{align*}

By the defintion of \Emulator,
EIMX's that differ only in \Vbool{z} will not appear
in the same Earley set,
and the number of AHFA states, \Vsize{fa}, is finite.
The theorem follows.
Therefore
$$ \size{\VVes{El}{i}} \le \size{fa} \times \size{\VVes{L}{i}} $$

From Theorem {t:emulator-equivalence},
we have
$$ \size{\VVes{Em}{i}} = \size{\VVes{El}{i}} $$

And by the congruence of \Emulator in Marpa mode
with Marpa,
we know there is a bijection from a partition of the
Earley items in \VVes{Em}[i] to \VVes{M}[i],
so that
$$ \size{\VVes{M}{i}} \le \size{\VVes{Em}{i}} $$

Collecting the last three relations, we have
$$ \size{\VVes{M}{i}} \le \size{fa} \times \size{\VVes{L}{i}} $$

And summing over $\var{i}, 0 \le \var{i} \le \var{n}$
where $\var{n} = \Vsize{w}$,
we see that
$$ \order{\tablesizen{\Marpa}} = \order{\tablesizen{\Leo}} $$
\end{proof}

\subsection{Amortized complexity of Marpa's Earley items}

\begin{theorem}\label{t:O1-per-eim}
All time in \Marpa{} can be allocated
to the Earley items,
and in such a way that processing each Earley item
requires \Oc{} time and space.
\end{theorem}

\begin{proof}
For some the operations of \Earley,
it is known that the amortized time 
per Earley item
is amortized $\order(1)$\cite[Vol. 1, pages 326-327]{AU1972}.
Inspection of the algorithm for \Marpa{} will show
that all space
and time
for any operations new to \Marpa
can be assigned to the Earley items in
an obvious way,
and trivially shown to be \Oc.
\end{proof}

\subsection{Marpa complexity}

All of the complexity results for \Leo are
based on the order of magnitude of the
sum of the cardinalities of its Earley sets.
From
Theorems \ref{t:eim-order} and \ref{t:O1-per-eim},
we see that the
order of magnitude of
the time
and space complexity of \Marpa{} is never worse than
that of \Leo.
Because
Theorem 4.3 of \cite[p. 172]{Leo1991}
shows that the
order of magnitude of
the time
and space complexity of \Leo is never worse than
that of \Earley,
we also see that the
order of magnitude of
the time
and space complexity of \Marpa{} is also
never worse than that of \Earley.

We now state some of the more important
specific results.
In the following theorems,
\var{n} is the length of the input, \Vsize{w}.

\begin{theorem}
For every LR-regular grammar,
\Marpa{} runs in $\order{n}$ time and space.
\end{theorem}

\begin{proof}
By Theorem 4.6 in \cite[p. 173]{Leo1991},
and Theorems \ref{t:eim-order} and \ref{t:O1-per-eim}.
\end{proof}

\begin{theorem}
For every unambiguous grammar,
\Marpa{} runs in $\order{n^2}$ time.
\end{theorem}

\begin{proof}
The time complexity of \Earley for unambiguous grammars
is $\order(n^2)$
(Theorem 4.10 in \cite[Vol 1, p. 327]{AU1972};
\cite{Earley 1971}).
The results follows by using
Theorem 4.3 of \cite[p. 172]{Leo1991},
and Theorems \ref{t:eim-order} and \ref{t:O1-per-eim}
\end{proof}

\begin{theorem}
For any context-free grammar,
\Marpa{} runs in $\order{n^3}$ time
and $\order{n^2}$ space.
\end{theorem}

\begin{proof}
The time complexity of \Earley is $\order(n^3)$
and its space complexity is $\order(n^2)$\cite{Earley 1971}.
The results follows by using
Theorem 4.3 of \cite[p. 172]{Leo1991},
and Theorems \ref{t:eim-order} and \ref{t:O1-per-eim}.
\end{proof}

\section{Generalizing the grammar}
\label{s:generalization}

As implemented,
Marpa generalizes the idea of grammars
and input streams beyond that so far described
for \Vg{} and \Vw.
Because the differences were
minor from a theoretical point of view,
and their discussion has been deferred to avoid
cluttering the proofs.
This section redefines \Vg{} and \Vw,
to incorporate the
deferred generalizations.

First, Marpa's grammars are in effect 3-tuples:
$$(\Vsymset{alphabet}, rules, \Vsym{start})$$
\Vsymset{term} is omitted, because
Marpa allows a symbol to be both a terminal
and a LHS.
This expansion of the grammar definition 
is made without loss of generalization,
or effect on the results.
\footnote{
Marpa has options which 
cause the traditional restrictions to
be enforced,
in part or in whole.
For error detection and efficiency,
users may well prefer this.
}

Second, Marpa's input model is a generalization of
the traditional input stream model
used so far.
Marpa's input is a set of tokens,
$toks$,
whose elements are triples of symbol,
start location and end location:
$$(\Vsym{t}, \Vloc{start}, \var{length})$$
such that
$$\var{length} \ge 1 \wedge \Vloc{start} \ge 0$$
The size of the input, \Vsize{toks} is the maximum over
\var{toks} of $\Vloc{start}+\var{length}$.
Tokens may overlap,
but gaps are not allowed:
\begin{align*}
    & \forall \Vloc{i} (\exists \token{t} \sep \token{t} \in \var{toks} \\
\wedge & t = (\Vsym{t}, \Vloc{start}, \var{length}) \\
\wedge & \Vloc{start} \le \Vloc{i} \le \Vloc{start}+\var{length}
\end{align*}

The traditional input stream is the special case of
a Marpa input stream where 
\begin{gather*}
\forall \token{tok} \sep \token{tok} \in \var{toks} \\
\implies \token{tok} = [\Vsym{s}, \Vloc{start}, 1]
\end{gather*}
and for all $\token{tok1}, \token{tok2}$ in \var{toks}
\begin{center}
\begin{tabular}{rl}
(i) & $\token{tok1} = [\sym{s1}, \loc{start1}, 1]$ \\
(ii) & $\token{tok2} = [\sym{s2}, \loc{start2}, 1]$ \\
(iii) & $\loc{start1} = \loc{start2} \implies \token{tok1} = \token{tok2}$ \\
\end{tabular}
\end{center}

The correctness results hold for Marpa input streams,
but to preserve the time complexity bounds,
certain restriction must be imposed on them.
Call an Earley set, $\es{table[j]}$
a "lookahead set" at $\loc{i}$
if $j>i \wedge$
and there exists some token $\token{tok} = (\sym{s}, \loc{start}, length)$
such that $\loc{start} \le i \wedge \loc{start}+length = \loc{j}$.
If, at every location,
the number of tokens which start there,
and the number of lookahead sets in play at that location
is less than a finite constant,
then Earley items can be added in amortized constant time
and the complexity results for
\Marpa{} stand.

\bibliographystyle{plain}

\begin{thebibliography}{10}

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Marpa-HTML}
Jeffrey~Kegler, 2011: Marpa-HTML.
\newblock [ Available online at http://search.cpan.org/dist/Marpa-HTML/. ]

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2012: Marpa-R2.
\newblock [ Available online at http://search.cpan.org/dist/Marpa-R2/. ]

\bibitem{Marpa-XS}
Jeffrey~Kegler, 2011: Marpa-XS-1.002000.
\newblock [ Available online at http://search.cpan.org/dist/Marpa-XS/. ]

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}

\tableofcontents
 
\end{document}
